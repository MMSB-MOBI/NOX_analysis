{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the seed data set\n",
    "\n",
    "Starting from complete trEMBL dataset <span style='background:#f7f3f7;padding:0.4em;border-radius:2px; border:solid bgrey 1px'>arwen:/mobi/group/NOX_CH/data/uniprot_trembl.fasta.gz</span> which is a symbolic link for `arwen:/mobi/group/databases/flat/uniprot_trembl_2019_02.fasta.gz`\n",
    " *  Split the dataset in small volumes\n",
    "     * script: <span style=\"color:green\">**split.py**</span>\n",
    "     * Usage:\n",
    "     Create and go to the `/mobi/group/NOX_GL/volumes` \n",
    "```console\n",
    "    ROOT_DIR=/mobi/group/NOX_CH\n",
    "    SCRIPT_DIR=/mobi/group/NOX_CH/nox-analysis/scripts\n",
    "    $SCRIPT_DIR/split.py $ROOT_DIR/data/uniprot_trembl.fasta.gz\n",
    "```\n",
    "\n",
    " * Run the HMMR and TMHMM annotations\n",
    "    * script: <span style=\"color:green\">**runHMMR_slurm.sh**</span>\n",
    "    * Usage:  \n",
    "  \n",
    "```console\n",
    "    mkdir $ROOT_DIR/seedSet\n",
    "    mkdir $ROOT_DIR/seedSet/work\n",
    "    $SCRIPT_DIR/runHMMR_slurm.sh $ROOT_DIR/volumes $ROOT_DIR/seedSet/work $ROOT_DIR/data/profiles\n",
    "```\n",
    "\n",
    " * Use this notebook to parse the _work_ folder (see **Parsing all data files** section)\n",
    "\n",
    "    * Filter-out non eukaryotic entries and dump the corresponding fasta sequence in folder <span style='background:#f7f3f7;padding:0.4em;border-radius:2px; border:solid bgrey 1px'>/mobi/group/NOX_CH/seedSet/NOX_noEukaryota</span> (create directory before)\n",
    "         \n",
    "\n",
    "\n",
    " * Preparing folders/sbatch scripts for pairwise N&W across the set of __NOX_noEukaryota__ fasta sequences\n",
    "    * script: <span style=\"color:green\">**runEMBOSS_slurm.sh**</span>\n",
    "    * Usage:\n",
    "```console\n",
    "mkdir $ROOT_DIR/seedSet/NOX_noEukaryota_needlePairwise_work\n",
    "$SCRIPT_DIR/runEMBOSS_slurm.sh $ROOT_DIR/seedSet/NOX_noEukaryota NOX_noEukaryota $ROOT_DIR/seedSet/NOX_noEukaryota_needlePairwise_work\n",
    "```\n",
    "\n",
    " * Concatenate all fasta sequences in a single file\n",
    "```console\n",
    "     cat $ROOT_DIR/seedSet/NOX_noEukaryota/*.fasta > $ROOT_DIR/seedSet/NOX_noEukaryota.mfasta\n",
    " ```\n",
    "\n",
    "* Perform full Pfam annotation\n",
    "\n",
    "```console\n",
    "     sbatch $SCRIPT_DIR/runHMMSCAN.sbatch /mobi/group/databases/hmmr/Pfam-A.hmm $ROOT_DIR/seedSet/NOX_noEukaryota.mfasta $ROOT_DIR/seedSet/NOX_noEukaryota_hmmscan.out\n",
    "```\n",
    "\n",
    "\n",
    " * Enrich the datacontainer with these new annotation, see **Read in additional PFAM annotations // Erase previous** section\n",
    "\n",
    "\n",
    " * Use the [Taxonomy notebook](http://localhost:8888/notebooks/NOX/Taxonomy.ipynb) to output a hierarchal tree\n",
    "     * link the output json file as $latest.json$\n",
    "\n",
    "\n",
    " * Start adhoc http server\n",
    "   \n",
    "   Go to `~/work/projects/NOX`\n",
    "```console\n",
    "node index.js\n",
    "``` \n",
    "\n",
    "* Visualize w/ D3 at `localhost:9615`\n",
    " \n",
    "### Creating the extended data set\n",
    "\n",
    "\n",
    "* Perform a psiblast on fasta files present in <span style='background:#f7f3f7;padding:0.4em;border-radius:2px; border:solid bgrey 1px'>arwen:/mobi/group/NOX_GL/seedSet/NOX_noEukaryota</span>\n",
    "\n",
    "    * Create the `extendedSet` folder\n",
    "\n",
    "    * script: <span style=\"color:green\">**runPsiBlast_slurm.sh**</span>\n",
    "    * Usage:\n",
    "```console\n",
    "$SCRIPT_DIR/runPsiBlast_slurm.sh $ROOT_DIR/seedSet/NOX_noEukaryota $ROOT_DIR/extendedSet/psiblastWork\n",
    "```\n",
    "\n",
    "* Browse all the psiblast workfolder and eliminate strictly identical proteins\n",
    "    * Go to `$ROOT/extendedSet`\n",
    "    * script:<span style=\"color:green\">**makePsiBlastNR.py**</span>\n",
    "    * Usage:\n",
    "```console\n",
    "python $SCRIPT_DIR/makePsiBlastNR.py ./psiblastWork ./NOX_noEukaryota_PB_NR.fasta > makePsiBlastNR.log\n",
    "```\n",
    "* Perform a full PFAM annotation\n",
    "```console\n",
    "hmmscan NOX_noEukaryota_PB_NR.fasta /mobi/group/databases/hmmr/Pfam-A.hmm > NOX_noEukaryota_PB_NR_hmmscan.out\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "sys.path.append(\"/Users/chilpert/Work/pyproteinsExt/src\")\n",
    "sys.path.append(\"/Users/chilpert/Work/pyproteins/src\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, io\n",
    "import urllib.request\n",
    "\n",
    "def mFastaParseZip(inputFile):\n",
    "    data = None\n",
    "    with io.TextIOWrapper(gzip.open(inputFile, 'r')) as f:\n",
    "        data = mFastaParseStream(f)\n",
    "    return data\n",
    "\n",
    "def mFastaParseUrl(url):\n",
    "    fp = urllib.request.urlopen(url)\n",
    "    mybytes = fp.read()\n",
    "    #mFastaParseStream(fp)\n",
    "    mystr = mybytes.decode(\"utf8\")\n",
    "    fp.close()\n",
    "    data = mFastaParseStream(mystr.split('\\n'))\n",
    "    \n",
    "#    print(mystr)\n",
    "    return data\n",
    "\n",
    "def mFastaParseStream(stream):\n",
    "    \n",
    "    data = {}    \n",
    "    headPtr = ''\n",
    "    for line in stream:\n",
    "        #print (line)\n",
    "        if line == '':\n",
    "            continue\n",
    "        s = line.replace('\\n','')\n",
    "        if s.startswith('>'):\n",
    "            headPtr = s.split()[0][1:]\n",
    "            \n",
    "            if headPtr in data:\n",
    "                raise ValueError('Smtg wrong')\n",
    "            data[headPtr] = {'header': s, 'sequence' : '' }\n",
    "            \n",
    "            continue\n",
    "        data[headPtr]['sequence'] += s\n",
    "    return data\n",
    "\n",
    "#mFastaParseUrl('http://www.uniprot.org/uniprot/S4Z6V5.fasta')\n",
    "#data = mFastaParse('/Volumes/arwen/home/ygestin/prositetask-backup/alignTrembl/bibl/Trembl_47/Trembl_47.fasta.gz')\n",
    "#test=None\n",
    "#with open('/Volumes/arwen/mobi/group/NOX_GL/work/uniprot_trembl_v11/hmmsearch.fasta', 'r') as f:\n",
    "#    test = mFastaParseStream(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def num(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return float(s)\n",
    "    \n",
    "    \n",
    "reTMH = re.compile('^(\\# ){0,1}([\\S]+)[\\s]+([\\S].*)[\\s]+([\\d\\.]+)$')\n",
    "def loadTMHMM(lDir):\n",
    "    \n",
    "    fastaContainer = None\n",
    "    with open( lDir+ '/hmmsearch.fasta', 'r') as f:\n",
    "        fastaContainer = mFastaParseStream(f)\n",
    "    \n",
    "    file = lDir+ '/tmhmm.out'\n",
    "    data = {}\n",
    "    with open(file, 'r') as f:\n",
    "        for l in f:\n",
    "            m = reTMH.search(l)\n",
    "            if m:\n",
    "                _id = m.groups()[1] \n",
    "                if _id not in data:\n",
    "                    if _id not in fastaContainer:\n",
    "                        raise ValueError(\"Misisng fasta for tmhmm prediction\")\n",
    "                    data[_id] = {'hCount':0 ,\n",
    "                                'helix':[], 'fasta' : fastaContainer[_id],\n",
    "                                'mask': '-' * len(fastaContainer[_id]['sequence'])\n",
    "                                }\n",
    "                \n",
    "                if not m.groups()[2].startswith('TMHMM2'):\n",
    "                    data[_id][re.sub('[\\s]*:[\\s]*$', '',m.groups()[2])] = num(m.groups()[3])\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                m2 = m.groups()[2].split('\\t')\n",
    "                if not m2:\n",
    "                    raise ValueError('could not parse helix line')\n",
    "                helixCoor =  {'volume' : m2[1], \n",
    "                              'start'  : num(m2[2].replace(' ', '')),\n",
    "                              'stop'   : num(m.groups()[3]) \n",
    "                            }\n",
    "                data[_id]['helix'].append(helixCoor)\n",
    "                \n",
    "                \n",
    "                data[_id]['helix'].append(helixCoor)\n",
    "                #print (data[_id]['mask']) \n",
    "                l_1 = len(data[_id]['mask'])\n",
    "                buf = list(data[_id]['mask'])\n",
    "                symbol = None\n",
    "                if helixCoor['volume'] == 'TMhelix':\n",
    "                    data[_id]['hCount'] += 1\n",
    "                    #symbol = 'H'\n",
    "                    symbol = str(data[_id]['hCount']) if data[_id]['hCount'] < 10 else str(data[_id]['hCount'])[-1]\n",
    "                elif helixCoor['volume'] == 'inside':\n",
    "                    symbol = 'i'\n",
    "                elif helixCoor['volume'] == 'outside':\n",
    "                    symbol = 'e'\n",
    "                else :\n",
    "                    raise ValueError(\"unknown symbol \" + helixCoor['volume'])\n",
    "\n",
    "                i=helixCoor['start'] - 1\n",
    "                j=helixCoor['stop']\n",
    "                #print(i,j,len(buf))\n",
    "                toAdd = symbol * (j - i)\n",
    "                buf[i:j] =  list(toAdd)#helixCoor['stop'] - helixCoor['start'] + 1\n",
    "                data[_id]['mask'] = ''.join(buf)\n",
    "                if len(data[_id]['mask']) != l_1:\n",
    "                    print(\"ERROR \", _id, l_1, len(data[_id]['mask']), '>>', i, j, '<<')\n",
    "                    print (len(buf[i:j]), len(list(toAdd)), symbol, '-->', toAdd )\n",
    "                #print(data[_id]['mask'])\n",
    "    \n",
    "    #        Hcluster(data)\n",
    "    return data\n",
    "#d = loadTMHMM('/Volumes/arwen/home/ygestin/prositetask-backup/alignTrembl/bibl/Trembl_47')\n",
    "#d = loadTMHMM('/Volumes/arwen/mobi/group/NOX_GL/work_sample/uniprot_trembl_v11')\n",
    "#d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HIS_clust(data, min=2, max=7):\n",
    "    for _id in data:\n",
    "        data[_id]['Htest'] = {'status' : False, 'data' : [] }\n",
    "\n",
    "        #Discard unwanted numbe of helices\n",
    "        if data[_id]['hCount'] < min or data[_id]['hCount'] > max:\n",
    "            #print('Wrong helices number ', _id, data[_id]['hCount'])\n",
    "            continue\n",
    "        \n",
    "        H_status = []\n",
    "        iMax = len(data[_id]['mask'])\n",
    "        # internal error check\n",
    "        if len(data[_id]['mask']) != len(data[_id]['fasta']['sequence']) :\n",
    "            print( len(data[_id]['mask']), len(data[_id]['fasta']['sequence']) )\n",
    "            print(_id, data[_id])\n",
    "            raise ValueError(\"\")\n",
    "        # Select only residues that are Histidine within TMH\n",
    "        for i in range(0, iMax):\n",
    "            if data[_id]['mask'][i] == \"i\" or  data[_id]['mask'][i] == \"e\":\n",
    "                continue\n",
    "            if not data[_id]['fasta']['sequence'][i] == \"H\":\n",
    "                continue\n",
    "            H_status.append( [i, data[_id]['mask'][i], False] )\n",
    "        # Pairwise comparaison between Histidine of the same helix, marking pairs separated by 12 to 14 residues\n",
    "        for i in range (0, len(H_status) - 1):\n",
    "            for j in range (i + 1, len(H_status)):\n",
    "                if H_status[i][1] != H_status[j][1]:\n",
    "                    continue\n",
    "                d = H_status[i][0] - H_status[j][0]\n",
    "                if d >= 12 or d <= 14:\n",
    "                    H_status[i][2] = True\n",
    "                    H_status[j][2] = True\n",
    "        \n",
    "        #print(H_status)\n",
    "        # Only keep marked histidine\n",
    "        H_status = [ x for x in H_status if x[2] ]\n",
    "        # Create a dicitinary where keys are Helices numbers\n",
    "        H_groups = {}\n",
    "        for x in H_status:\n",
    "            if not x[2]:\n",
    "                continue\n",
    "            if x[1] not in H_groups:\n",
    "                H_groups[x[1]]=[]\n",
    "            H_groups[x[1]].append(x)\n",
    "        \n",
    "        # The test is passed if at least two distinct helices feature at least one correctly spaced histidine pair\n",
    "        # ie : if the helice dictionary has more than 1 entrie\n",
    "        #print(H_status)\n",
    "        #print(\"-->\", H_groups)\n",
    "        HisTestBool = True if len(H_groups) > 1 else False\n",
    "        \n",
    "        data[_id]['Htest']['status'] = HisTestBool\n",
    "        data[_id]['Htest']['data'] = H_groups\n",
    "    return data\n",
    "\n",
    "#m = HIS_clust(d)\n",
    "#print(len([ m[x] for x in m if m[x]['Htest']['status'] ]), len(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, time\n",
    "import time\n",
    "\n",
    "def save(data, tag=None):\n",
    "    saveDir=\"/Volumes/arwen/mobi/group/NOX_CH/pickle_saved\"\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    fTag = \"NOX_annotation_\" + tag + \"_\" if tag else \"NOX_annotation_\"\n",
    "    fSerialDump = fTag + timestr + \".pickle\"\n",
    "    with open(saveDir + '/' + fSerialDump, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print('data structure saved to', saveDir + '/' + fSerialDump)\n",
    "\n",
    "def load(fileName):\n",
    "    saveDir=\"/Volumes/arwen/mobi/group/NOX_CH/pickle_saved\"\n",
    "    d = pickle.load( open(saveDir + \"/\" + fileName, \"rb\" ) )\n",
    "    print(\"restore a annotated container of \", len(d), \"elements\")\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing all data files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing HMMR data\n",
    "NB: There are stdout of 3 consecutive hmmr calls\n",
    "\n",
    "All in a single **data** container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "Warning:: >> tr|A0A0F0DDW2|A0A0F0DDW2_9PSED  Cytochrome B561 OS=Pseudomonas sp. 21 OX=1619948 GN=UB43_27990 PE=4 SV=1\n",
      "   [No individual domains that satisfy reporting thresholds (although complete target did)]\n",
      "\n",
      "\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n"
     ]
    }
   ],
   "source": [
    "import pyproteinsExt.hmmrContainerFactory as hm\n",
    "import glob\n",
    "dataDir=glob.glob('/Volumes/arwen/mobi/group/NOX_CH/seedSet/work/uniprot_trembl_v*')\n",
    "\n",
    "data = hm.parse(inputFile=dataDir[0] + '/hmmsearch.out')\n",
    "i=0\n",
    "\n",
    "for iDir in dataDir[1:]:\n",
    "    #print(iDir)\n",
    "    data += hm.parse(inputFile=iDir + '/hmmsearch.out')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Discard domains with evalue > 1e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.evalue_filter(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.evalue_filter(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257799\n",
      "98292\n"
     ]
    }
   ],
   "source": [
    "T_all=data.T()\n",
    "T=data.T(filter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proteins with at least 1 domain 178539\n",
      "Proteins with at least 1 domain with evalue filter at 1e-3 64021\n"
     ]
    }
   ],
   "source": [
    "print(\"Proteins with at least 1 domain\",len(T_all))\n",
    "print(\"Proteins with at least 1 domain with evalue filter at 1e-3\",len(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading TMHMM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataTMHMM = {}\n",
    "for lDir in dataDir:\n",
    "    d = loadTMHMM(lDir)\n",
    "    if set( dataTMHMM.keys() ) & set( d.keys() ):\n",
    "        print('doublons')\n",
    "    dataTMHMM.update(d)\n",
    "\n",
    "dataTMHMM = HIS_clust(dataTMHMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform a PFAM domain indexed data structure in a protein indexed data structure\n",
    "Then filter out the protein that feature the 3 domains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins entries featuring FAD 77203\n",
      "Number of proteins entries featuring NAD 121386\n",
      "Number of proteins entries featuring Ferric reductase 59209\n",
      "Size of their intersection 18020\n"
     ]
    }
   ],
   "source": [
    "D_all = {}\n",
    "fad=0\n",
    "nad=0\n",
    "ferric=0\n",
    "for protein in T_all:\n",
    "    if len(T_all[protein]) == 3:\n",
    "           D_all[protein] = T_all[protein]\n",
    "    for dom in T_all[protein]: \n",
    "        if dom == \"PF08022_full\":\n",
    "            fad+=1\n",
    "        elif dom == \"PF01794_full\": \n",
    "            ferric+=1\n",
    "        elif dom == \"PF08030_full\": \n",
    "            nad+=1\n",
    "        else: \n",
    "            print(\"OOOO\")\n",
    "        #if dom == \"PF08022_full\":\n",
    "            \n",
    "print('Number of proteins entries featuring FAD',fad)\n",
    "print('Number of proteins entries featuring NAD',nad)\n",
    "print('Number of proteins entries featuring Ferric reductase',ferric)\n",
    "print('Size of their intersection',len(D_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins entries featuring FAD 31593\n",
      "Number of proteins entries featuring NAD 37456\n",
      "Number of proteins entries featuring Ferric reductase 29243\n",
      "Size of their intersection 14022\n"
     ]
    }
   ],
   "source": [
    "D = {}\n",
    "fad=0\n",
    "nad=0\n",
    "ferric=0\n",
    "for protein in T:\n",
    "    if len(T[protein]) == 3:\n",
    "           D[protein] = T[protein]\n",
    "    for dom in T[protein]: \n",
    "        if dom == \"PF08022_full\":\n",
    "            fad+=1\n",
    "        elif dom == \"PF01794_full\": \n",
    "            ferric+=1\n",
    "        elif dom == \"PF08030_full\": \n",
    "            nad+=1\n",
    "        else: \n",
    "            print(\"OOOO\")\n",
    "        #if dom == \"PF08022_full\":\n",
    "            \n",
    "print('Number of proteins entries featuring FAD',fad)\n",
    "print('Number of proteins entries featuring NAD',nad)\n",
    "print('Number of proteins entries featuring Ferric reductase',ferric)\n",
    "print('Size of their intersection',len(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge TMHMM & HMMR data\n",
    "\n",
    "  * Proteins with the 3 domain types\n",
    "  * Their TMHMM status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of protein entries featuring FAD,NAD and Ferric transferase domains 14022\n",
      "Number of protein featuring 2 to 7 TMH and 2 bi-histine 178540\n",
      "Size of their intersection 5972\n"
     ]
    }
   ],
   "source": [
    "merged_all = {}\n",
    "for _id in D_all:\n",
    "    if _id not in dataTMHMM:\n",
    "        print('Missing protein ID' + _id)\n",
    "    if not dataTMHMM[_id]['Htest']['status']:\n",
    "        continue\n",
    "    merged_all[_id] = {\n",
    "        'hmmr' : D_all[_id],\n",
    "        'tmhmm' : dataTMHMM[_id]\n",
    "    }\n",
    "    \n",
    "print('Number of protein entries featuring FAD,NAD and Ferric transferase domains', len(D))\n",
    "print('Number of protein featuring 2 to 7 TMH and 2 bi-histine', len(dataTMHMM))\n",
    "print('Size of their intersection', len(merged_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of protein entries featuring FAD,NAD and Ferric transferase domains 14022\n",
      "Number of protein featuring 2 to 7 TMH and 2 bi-histine 178540\n",
      "Size of their intersection 5043\n"
     ]
    }
   ],
   "source": [
    "merged = {}\n",
    "for _id in D:\n",
    "    if _id not in dataTMHMM:\n",
    "        print('Missing protein ID' + _id)\n",
    "    if not dataTMHMM[_id]['Htest']['status']:\n",
    "        continue\n",
    "    merged[_id] = {\n",
    "        'hmmr' : D[_id],\n",
    "        'tmhmm' : dataTMHMM[_id]\n",
    "    }\n",
    "    \n",
    "print('Number of protein entries featuring FAD,NAD and Ferric transferase domains', len(D))\n",
    "print('Number of protein featuring 2 to 7 TMH and 2 bi-histine', len(dataTMHMM))\n",
    "print('Size of their intersection', len(merged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discard Eukaryota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract TaxonID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTaxID(datum):\n",
    "    reTaxID = re.compile('OX=([\\d]+)')\n",
    "    m = reTaxID.search(datum['tmhmm']['fasta']['header'])\n",
    "    if not m:\n",
    "        raise ValueError('Cant parse taxid from', datum['tmhmm']['fasta']['header'])\n",
    "    datum['taxid'] = m.groups()[0]\n",
    "    \n",
    "for _id in merged:\n",
    "    getTaxID(merged[_id])\n",
    "\n",
    "for _id in merged_all:\n",
    "    getTaxID(merged_all[_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flag Non Eukaryota phylum members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import NCBITaxa\n",
    "ncbi=NCBITaxa() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eukaryota 5116\n",
      "Bacteria 848\n",
      "Archaea 3\n",
      "Unclassified 2\n",
      "Not found 3\n"
     ]
    }
   ],
   "source": [
    "unclassified=0\n",
    "archaea=0\n",
    "bacteria=0\n",
    "eukaryota=0\n",
    "not_found=0\n",
    "for _id in merged_all: \n",
    "    bool=True\n",
    "    taxid=merged_all[_id]['taxid']\n",
    "    #print(taxid)\n",
    "    try : \n",
    "        lineage=ncbi.get_lineage(taxid)\n",
    "        lineage_rank=ncbi.get_rank(lineage)\n",
    "        superkingdom=[taxid for taxid in lineage_rank if lineage_rank[taxid]=='superkingdom']\n",
    "        if superkingdom : \n",
    "            name=ncbi.get_taxid_translator(superkingdom)[superkingdom[0]]\n",
    "            if name == \"Eukaryota\":\n",
    "                bool=False\n",
    "                eukaryota+=1\n",
    "            elif name == \"Bacteria\":\n",
    "                bacteria+=1\n",
    "            elif name == \"Archaea\": \n",
    "                archaea+=1\n",
    "            else: \n",
    "                print(\"OOO\")\n",
    "        else: \n",
    "            unclassified+=1\n",
    "        merged_all[_id]['isNoEukaryota']=bool\n",
    "            \n",
    "    except : \n",
    "        not_found+=1\n",
    "\n",
    "print(\"Eukaryota\",eukaryota)\n",
    "print(\"Bacteria\",bacteria)\n",
    "print(\"Archaea\",archaea)\n",
    "print(\"Unclassified\",unclassified)\n",
    "print(\"Not found\", not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtered data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eukaryota 4870\n",
      "Bacteria 169\n",
      "Archaea 0\n",
      "Unclassified 1\n",
      "Not found 3\n"
     ]
    }
   ],
   "source": [
    "unclassified=0\n",
    "archaea=0\n",
    "bacteria=0\n",
    "eukaryota=0\n",
    "not_found=0\n",
    "for _id in merged: \n",
    "    bool=True\n",
    "    taxid=merged[_id]['taxid']\n",
    "    #print(taxid)\n",
    "    try : \n",
    "        lineage=ncbi.get_lineage(taxid)\n",
    "        lineage_rank=ncbi.get_rank(lineage)\n",
    "        superkingdom=[taxid for taxid in lineage_rank if lineage_rank[taxid]=='superkingdom']\n",
    "        if superkingdom : \n",
    "            name=ncbi.get_taxid_translator(superkingdom)[superkingdom[0]]\n",
    "            if name == \"Eukaryota\":\n",
    "                bool=False\n",
    "                eukaryota+=1\n",
    "            elif name == \"Bacteria\":\n",
    "                bacteria+=1\n",
    "            elif name == \"Archaea\": \n",
    "                archaea+=1\n",
    "            else: \n",
    "                print(\"OOO\")\n",
    "        else: \n",
    "            unclassified+=1\n",
    "        merged[_id]['isNoEukaryota']=bool\n",
    "            \n",
    "    except : \n",
    "        not_found+=1\n",
    "\n",
    "print(\"Eukaryota\",eukaryota)\n",
    "print(\"Bacteria\",bacteria)\n",
    "print(\"Archaea\",archaea)\n",
    "print(\"Unclassified\",unclassified)\n",
    "print(\"Not found\", not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_filter_20190516-163544.pickle\n"
     ]
    }
   ],
   "source": [
    "save(merged,\"filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just keep non Eukaryota sequences in datacontainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=merged_all\n",
    "data_noEuk_all={}\n",
    "for k in data:\n",
    "    if not 'isNoEukaryota' in data[k]:\n",
    "        continue\n",
    "    if data[k]['isNoEukaryota']:\n",
    "        data_noEuk_all[k] = data[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=merged\n",
    "data_noEuk={}\n",
    "for k in data:\n",
    "    if not 'isNoEukaryota' in data[k]:\n",
    "        continue\n",
    "    if data[k]['isNoEukaryota']:\n",
    "        data_noEuk[k] = data[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853 NOX proteins found with no filter.\n",
      "170 NOX proteins found with evalue filter\n"
     ]
    }
   ],
   "source": [
    "print(len(data_noEuk_all),\"NOX proteins found with no filter.\")\n",
    "print(len(data_noEuk),\"NOX proteins found with evalue filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins_all=set([p for p in data_noEuk_all])\n",
    "proteins_filtered=set([p for p in data_noEuk])\n",
    "deleted_proteins=proteins_all.difference(proteins_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write deleted proteins with their taxonomy in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=open(\"/Volumes/arwen/mobi/group/NOX_CH/deleted_proteins_with_domain_filter.tsv\",'w')\n",
    "o.write(\"#Protein\\tTaxid\\tTaxname\\n\")\n",
    "for p in deleted_proteins: \n",
    "    taxid=data_noEuk_all[p]['taxid']\n",
    "    taxname=ncbi.get_taxid_translator([taxid]).get(int(taxid),\"no name\")\n",
    "    o.write(p+\"\\t\"+taxid+\"\\t\"+taxname+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_filter_noEukaryota_20190516-163551.pickle\n"
     ]
    }
   ],
   "source": [
    "save(data_noEuk,\"filter_noEukaryota\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we work only with filtered proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter obsolete Uniprot entry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore a annotated container of  170 elements\n"
     ]
    }
   ],
   "source": [
    "data=load(\"NOX_annotation_filter_noEukaryota_20190516-163551.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing cache location to /Users/chilpert/cache/uniprot\n",
      "Reindexing /Users/chilpert/cache/uniprot\n",
      "Acknowledged 163 entries (/Users/chilpert/cache/uniprot)\n",
      "Changing cache location to /Users/chilpert/cache/pfam\n",
      "Reindexing /Users/chilpert/cache/pfam\n",
      "Acknowledged 159 entries (/Users/chilpert/cache/pfam)\n",
      "got to fetch A0A3L9B3M0\n",
      "got to fetch A0A2Z5AMG3\n",
      "got to fetch A0A168SHW4\n",
      "got to fetch A0A3L4QTP8\n",
      "got to fetch A0A3L2F2W5\n",
      "got to fetch A0A3L3SJU7\n",
      "got to fetch A0A3L3SPV6\n"
     ]
    }
   ],
   "source": [
    "import pyproteinsExt.uniprot as uniprot\n",
    "uColl = uniprot.getUniprotCollection()\n",
    "uColl.setCache(location=\"/Users/chilpert/cache/uniprot\")\n",
    "uniprot.getPfamCollection().setCache(location=\"/Users/chilpert/cache/pfam\")\n",
    "new_data={}\n",
    "c=0\n",
    "not_found=[]\n",
    "for p in data :\n",
    "    p_id=p.split(\"|\")[1]\n",
    "    try : \n",
    "        obj=uColl.get(p_id)\n",
    "        new_data[p]=data[p]\n",
    "        new_data[p]['RefSeq']={}\n",
    "        new_data[p]['RefSeq']['genome']=obj.Genome.RefSeqRef\n",
    "        new_data[p]['RefSeq']['protein']=obj.Genome.RefSeqProteinRef\n",
    "        new_data[p]['EMBL']={}\n",
    "        new_data[p]['EMBL']['genome']=obj.Genome.EMBLRef \n",
    "        new_data[p]['EMBL']['protein']=obj.Genome.EMBLProteinRef\n",
    "        #new_data[p]['Uniprot_domains']=obj.domains\n",
    "        \n",
    "    except : \n",
    "        c+=1\n",
    "        not_found.append(p_id)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 with Uniprot entry.\n"
     ]
    }
   ],
   "source": [
    "print(len(new_data),\"with Uniprot entry.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_noEukaryota_noObsolete_20190516-172955.pickle\n"
     ]
    }
   ],
   "source": [
    "save(new_data,\"noEukaryota_noObsolete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save non Eukaryota sequences in given directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore a annotated container of  163 elements\n"
     ]
    }
   ],
   "source": [
    "data=load(\"NOX_annotation_noEukaryota_noObsolete_20190516-172955.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "saveDir=\"/Volumes/arwen/mobi/group/NOX_CH/seedSet/NOX_noEukaryota\"\n",
    "def mFastaSplitDump(data, saveDir, fileTag='default' ,distinct=True):\n",
    "    c = 1\n",
    "    f = None\n",
    "    if not distinct:\n",
    "        f = open(saveDir + '/'+ fileTag + '_all.fasta', 'w')\n",
    "        \n",
    "    for _id in data:\n",
    "        if distinct:\n",
    "            f = open(saveDir + '/'+ fileTag + '_' + str(c) + '.fasta', 'w')\n",
    "        c += 1\n",
    "        f.write(data[_id]['tmhmm']['fasta']['header'])\n",
    "        f.write(re.sub(\"(.{81})\", \"\\\\1\\n\", data[_id]['tmhmm']['fasta']['sequence'], 0, re.DOTALL))\n",
    "        if distinct:\n",
    "            f.close()\n",
    "    if not distinct:    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "mFastaSplitDump(data, saveDir, 'NOX_noEukaryota')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Pfam annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse hmm and discard domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproteinsExt.hmmrContainerFactory as hm\n",
    "\n",
    "def parse_and_discard(data,evalue_threshold=None): \n",
    "    fileName=\"/Volumes/arwen/mobi/group/NOX_CH/seedSet/NOX_noEukaryota_hmmscan.out\" \n",
    "    hscan=hm.parse(inputFile=fileName)\n",
    "    bool_filter=False\n",
    "    if evalue_threshold: \n",
    "        hscan.evalue_filter(evalue_threshold)\n",
    "        bool_filter=True \n",
    "    for e in hscan.T(filter=bool_filter): \n",
    "        data[e]['hmmr']=hscan.T(filter=bool_filter)[e]\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproteinsExt.hmmrContainerFactory as hm\n",
    "fileName=\"/Volumes/arwen/mobi/group/NOX_CH/seedSet/NOX_noEukaryota_hmmscan.out\"\n",
    "#fileName=\"/tmp/hmmscan.out\"\n",
    "hscan = hm.parse(inputFile=fileName)\n",
    "hscan.evalue_filter(1e-3)\n",
    "for e in hscan.T(filter=True):\n",
    "    data[e]['hmmr'] = hscan.T(filter=True)[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalue threshold 1e-3\n",
    "data10_3=parse_and_discard(data,1e-3)\n",
    "# Evalue threshold 1e-1\n",
    "data10_1=parse_and_discard(data,1e-1)\n",
    "# All \n",
    "data_all=parse_and_discard(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_filter_fullPfam_filteredDomains1e-3_20190517-095430.pickle\n",
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_filter_fullPfam_filteredDomains1e-1_20190517-095431.pickle\n",
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_filter_fullPfam_allDomains_20190517-095431.pickle\n"
     ]
    }
   ],
   "source": [
    "save(data10_3,\"filter_fullPfam_filteredDomains1e-3\")\n",
    "save(data10_1,\"filter_fullPfam_filteredDomains1e-1\")\n",
    "save(data_all,\"filter_fullPfam_allDomains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of filtered sequence 386\n",
      "Number of negative to:\n",
      "*The NAD pattern 54 \n",
      "*The FAD pattern 147 \n",
      "*Both patterns  16\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "reMotifNADPH = re.compile('G[ISVL]G[VIAF][TAS][PYTA]')\n",
    "reMotifFAD = re.compile('H[PSA]F[TS][LIMV]')\n",
    "\n",
    "NAD_miss = 0\n",
    "FAD_miss = 0\n",
    "Both_miss = 0\n",
    "for p in merged_restore:\n",
    "    seq = merged_restore[p]['tmhmm']['fasta']['sequence']\n",
    "    m = reMotifNADPH.search(seq)\n",
    "    n = reMotifFAD.search(seq)\n",
    "    merged_restore[p]['NADPH_reg'] = True if m else False\n",
    "    merged_restore[p]['FAD_reg']   = True if n else False\n",
    "\n",
    "    if not m:\n",
    "        NAD_miss += 1\n",
    "        if not n:\n",
    "            Both_miss += 1\n",
    "    if not n:\n",
    "        FAD_miss += 1\n",
    "\n",
    "print('Total Number of filtered sequence', len(merged_restore))\n",
    "print('Number of negative to:')\n",
    "print('*The NAD pattern',str(NAD_miss), '\\n*The FAD pattern', str(FAD_miss), '\\n*Both patterns ', Both_miss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
