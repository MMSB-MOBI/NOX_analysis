{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the seed data set\n",
    "\n",
    "Starting from complete trEMBL dataset <span style='background:#f7f3f7;padding:0.4em;border-radius:2px; border:solid bgrey 1px'>arwen:/mobi/group/NOX_CH/data/uniprot_trembl.fasta.gz</span> which is a symbolic link for `arwen:/mobi/group/databases/flat/uniprot_trembl_2019_02.fasta.gz`\n",
    " *  Split the dataset in small volumes\n",
    "     * script: <span style=\"color:green\">**split.py**</span>\n",
    "     * Usage:\n",
    "     Create and go to the `/mobi/group/NOX_GL/volumes` \n",
    "```console\n",
    "    ROOT_DIR=/mobi/group/NOX_CH\n",
    "    SCRIPT_DIR=/mobi/group/NOX_CH/nox-analysis/scripts\n",
    "    $SCRIPT_DIR/split.py $ROOT_DIR/data/uniprot_trembl.fasta.gz\n",
    "```\n",
    "\n",
    " * Run the HMMR and TMHMM annotations\n",
    "    * script: <span style=\"color:green\">**runHMMR_slurm.sh**</span>\n",
    "    * Usage:  \n",
    "  \n",
    "```console\n",
    "    mkdir $ROOT_DIR/seedSet\n",
    "    mkdir $ROOT_DIR/seedSet/work\n",
    "    $SCRIPT_DIR/runHMMR_slurm.sh $ROOT_DIR/volumes $ROOT_DIR/seedSet/work $ROOT_DIR/data/profiles\n",
    "```\n",
    "\n",
    " * Use this notebook to parse the _work_ folder (see **Parsing all data files** section)\n",
    "\n",
    "    * Filter-out non eukaryotic entries and dump the corresponding fasta sequence in folder <span style='background:#f7f3f7;padding:0.4em;border-radius:2px; border:solid bgrey 1px'>/mobi/group/NOX_CH/seedSet/NOX_noEukaryota</span> (create directory before)\n",
    "         \n",
    "\n",
    "\n",
    " * Preparing folders/sbatch scripts for pairwise N&W across the set of __NOX_noEukaryota__ fasta sequences\n",
    "    * script: <span style=\"color:green\">**runEMBOSS_slurm.sh**</span>\n",
    "    * Usage:\n",
    "```console\n",
    "mkdir $ROOT_DIR/seedSet/NOX_noEukaryota_needlePairwise_work\n",
    "$SCRIPT_DIR/runEMBOSS_slurm.sh $ROOT_DIR/seedSet/NOX_noEukaryota NOX_noEukaryota $ROOT_DIR/seedSet/NOX_noEukaryota_needlePairwise_work\n",
    "```\n",
    "\n",
    " * Concatenate all fasta sequences in a single file, clusters redundant sequences\n",
    "```console\n",
    "     cat $ROOT_DIR/seedSet/NOX_noEukaryota/*.fasta > $ROOT_DIR/seedSet/NOX_noEukaryota.mfasta\n",
    "    mmseqs createdb /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota.mfasta /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_mmseqsdb \n",
    "    mmseqs cluster /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_mmseqsdb /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_clust100 /Volumes/arwen$ROOT_DIR/seedSet/tmp_NOX_noEukaryota_clust100 -c 1 \n",
    "    mmseqs createtsv /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_mmseqsdb /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_mmseqsdb  /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_clust100  /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_clust100.tsv --full-header\n",
    "    mmseqs result2repseq /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_mmseqsdb /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_clust100 /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_clust100_seq \n",
    "    mmseqs result2flat /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_mmseqsdb /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_mmseqsdb /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_clust100_seq  /Volumes/arwen$ROOT_DIR/seedSet/NOX_noEukaryota_clust100.fasta --use-fasta-header\n",
    " ```\n",
    " \n",
    "* Enrich the datacontainer with redundant information, see **Add redundant informations** section   \n",
    "\n",
    "\n",
    "* Perform full Pfam annotation\n",
    "\n",
    "```console\n",
    "     sbatch $SCRIPT_DIR/runHMMSCAN.sbatch /mobi/group/databases/hmmr/Pfam-A.hmm $ROOT_DIR/seedSet/NOX_noEukaryota.mfasta $ROOT_DIR/seedSet/NOX_noEukaryota_hmmscan.out\n",
    "```\n",
    "\n",
    "\n",
    " * Enrich the datacontainer with these new annotation, see **Read in additional PFAM annotations // Erase previous** section\n",
    "\n",
    "\n",
    " * Use the [Taxonomy notebook](http://localhost:8888/notebooks/NOX/Taxonomy.ipynb) to output a hierarchal tree\n",
    "     * link the output json file as $latest.json$\n",
    "\n",
    "\n",
    " * Start adhoc http server\n",
    "   \n",
    "   Go to `~/work/projects/NOX`\n",
    "```console\n",
    "node index.js\n",
    "``` \n",
    "\n",
    "* Visualize w/ D3 at `localhost:9615`\n",
    " \n",
    "### Creating the extended data set\n",
    "\n",
    "\n",
    "* Perform a psiblast on fasta files present in <span style='background:#f7f3f7;padding:0.4em;border-radius:2px; border:solid bgrey 1px'>arwen:/mobi/group/NOX_GL/seedSet/NOX_noEukaryota</span>\n",
    "\n",
    "    * Create the `extendedSet` folder\n",
    "\n",
    "    * script: <span style=\"color:green\">**runPsiBlast_slurm.sh**</span>\n",
    "    * Usage:\n",
    "```console\n",
    "$SCRIPT_DIR/runPsiBlast_slurm.sh $ROOT_DIR/seedSet/NOX_noEukaryota $ROOT_DIR/extendedSet/psiblastWork\n",
    "```\n",
    "\n",
    "* Browse all the psiblast workfolder and eliminate strictly identical proteins\n",
    "    * Go to `$ROOT/extendedSet`\n",
    "    * script:<span style=\"color:green\">**makePsiBlastNR.py**</span>\n",
    "    * Usage:\n",
    "```console\n",
    "python $SCRIPT_DIR/makePsiBlastNR.py ./psiblastWork ./NOX_noEukaryota_PB_NR.fasta > makePsiBlastNR.log\n",
    "```\n",
    "* Perform a full PFAM annotation\n",
    "```console\n",
    "hmmscan NOX_noEukaryota_PB_NR.fasta /mobi/group/databases/hmmr/Pfam-A.hmm > NOX_noEukaryota_PB_NR_hmmscan.out\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "sys.path.append(\"/Users/chilpert/Work/pyproteinsExt/src\")\n",
    "sys.path.append(\"/Users/chilpert/Work/pyproteins/src\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, io\n",
    "import urllib.request\n",
    "\n",
    "def mFastaParseZip(inputFile):\n",
    "    data = None\n",
    "    with io.TextIOWrapper(gzip.open(inputFile, 'r')) as f:\n",
    "        data = mFastaParseStream(f)\n",
    "    return data\n",
    "\n",
    "def mFastaParseUrl(url):\n",
    "    fp = urllib.request.urlopen(url)\n",
    "    mybytes = fp.read()\n",
    "    #mFastaParseStream(fp)\n",
    "    mystr = mybytes.decode(\"utf8\")\n",
    "    fp.close()\n",
    "    data = mFastaParseStream(mystr.split('\\n'))\n",
    "    \n",
    "#    print(mystr)\n",
    "    return data\n",
    "\n",
    "def mFastaParseStream(stream):\n",
    "    \n",
    "    data = {}    \n",
    "    headPtr = ''\n",
    "    for line in stream:\n",
    "        #print (line)\n",
    "        if line == '':\n",
    "            continue\n",
    "        s = line.replace('\\n','')\n",
    "        if s.startswith('>'):\n",
    "            headPtr = s.split()[0][1:]\n",
    "            \n",
    "            if headPtr in data:\n",
    "                raise ValueError('Smtg wrong')\n",
    "            data[headPtr] = {'header': s, 'sequence' : '' }\n",
    "            \n",
    "            continue\n",
    "        data[headPtr]['sequence'] += s\n",
    "    return data\n",
    "\n",
    "#mFastaParseUrl('http://www.uniprot.org/uniprot/S4Z6V5.fasta')\n",
    "#data = mFastaParse('/Volumes/arwen/home/ygestin/prositetask-backup/alignTrembl/bibl/Trembl_47/Trembl_47.fasta.gz')\n",
    "#test=None\n",
    "#with open('/Volumes/arwen/mobi/group/NOX_GL/work/uniprot_trembl_v11/hmmsearch.fasta', 'r') as f:\n",
    "#    test = mFastaParseStream(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def num(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return float(s)\n",
    "    \n",
    "    \n",
    "reTMH = re.compile('^(\\# ){0,1}([\\S]+)[\\s]+([\\S].*)[\\s]+([\\d\\.]+)$')\n",
    "def loadTMHMM(lDir):\n",
    "    \n",
    "    fastaContainer = None\n",
    "    with open( lDir+ '/hmmsearch.fasta', 'r') as f:\n",
    "        fastaContainer = mFastaParseStream(f)\n",
    "    \n",
    "    file = lDir+ '/tmhmm.out'\n",
    "    data = {}\n",
    "    with open(file, 'r') as f:\n",
    "        for l in f:\n",
    "            m = reTMH.search(l)\n",
    "            if m:\n",
    "                _id = m.groups()[1] \n",
    "                if _id not in data:\n",
    "                    if _id not in fastaContainer:\n",
    "                        raise ValueError(\"Misisng fasta for tmhmm prediction\")\n",
    "                    data[_id] = {'hCount':0 ,\n",
    "                                'helix':[], 'fasta' : fastaContainer[_id],\n",
    "                                'mask': '-' * len(fastaContainer[_id]['sequence'])\n",
    "                                }\n",
    "                \n",
    "                if not m.groups()[2].startswith('TMHMM2'):\n",
    "                    data[_id][re.sub('[\\s]*:[\\s]*$', '',m.groups()[2])] = num(m.groups()[3])\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                m2 = m.groups()[2].split('\\t')\n",
    "                if not m2:\n",
    "                    raise ValueError('could not parse helix line')\n",
    "                helixCoor =  {'volume' : m2[1], \n",
    "                              'start'  : num(m2[2].replace(' ', '')),\n",
    "                              'stop'   : num(m.groups()[3]) \n",
    "                            }\n",
    "                data[_id]['helix'].append(helixCoor)\n",
    "                \n",
    "                \n",
    "                data[_id]['helix'].append(helixCoor)\n",
    "                #print (data[_id]['mask']) \n",
    "                l_1 = len(data[_id]['mask'])\n",
    "                buf = list(data[_id]['mask'])\n",
    "                symbol = None\n",
    "                if helixCoor['volume'] == 'TMhelix':\n",
    "                    data[_id]['hCount'] += 1\n",
    "                    #symbol = 'H'\n",
    "                    symbol = str(data[_id]['hCount']) if data[_id]['hCount'] < 10 else str(data[_id]['hCount'])[-1]\n",
    "                elif helixCoor['volume'] == 'inside':\n",
    "                    symbol = 'i'\n",
    "                elif helixCoor['volume'] == 'outside':\n",
    "                    symbol = 'e'\n",
    "                else :\n",
    "                    raise ValueError(\"unknown symbol \" + helixCoor['volume'])\n",
    "\n",
    "                i=helixCoor['start'] - 1\n",
    "                j=helixCoor['stop']\n",
    "                #print(i,j,len(buf))\n",
    "                toAdd = symbol * (j - i)\n",
    "                buf[i:j] =  list(toAdd)#helixCoor['stop'] - helixCoor['start'] + 1\n",
    "                data[_id]['mask'] = ''.join(buf)\n",
    "                if len(data[_id]['mask']) != l_1:\n",
    "                    print(\"ERROR \", _id, l_1, len(data[_id]['mask']), '>>', i, j, '<<')\n",
    "                    print (len(buf[i:j]), len(list(toAdd)), symbol, '-->', toAdd )\n",
    "                #print(data[_id]['mask'])\n",
    "    \n",
    "    #        Hcluster(data)\n",
    "    return data\n",
    "#d = loadTMHMM('/Volumes/arwen/home/ygestin/prositetask-backup/alignTrembl/bibl/Trembl_47')\n",
    "#d = loadTMHMM('/Volumes/arwen/mobi/group/NOX_GL/work_sample/uniprot_trembl_v11')\n",
    "#d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HIS_clust(data, min=2, max=7):\n",
    "    for _id in data:\n",
    "        data[_id]['Htest'] = {'status' : False, 'data' : [] }\n",
    "\n",
    "        #Discard unwanted numbe of helices\n",
    "        if data[_id]['hCount'] < min or data[_id]['hCount'] > max:\n",
    "            #print('Wrong helices number ', _id, data[_id]['hCount'])\n",
    "            continue\n",
    "        \n",
    "        H_status = []\n",
    "        iMax = len(data[_id]['mask'])\n",
    "        # internal error check\n",
    "        if len(data[_id]['mask']) != len(data[_id]['fasta']['sequence']) :\n",
    "            print( len(data[_id]['mask']), len(data[_id]['fasta']['sequence']) )\n",
    "            print(_id, data[_id])\n",
    "            raise ValueError(\"\")\n",
    "        # Select only residues that are Histidine within TMH\n",
    "        for i in range(0, iMax):\n",
    "            if data[_id]['mask'][i] == \"i\" or  data[_id]['mask'][i] == \"e\":\n",
    "                continue\n",
    "            if not data[_id]['fasta']['sequence'][i] == \"H\":\n",
    "                continue\n",
    "            H_status.append( [i, data[_id]['mask'][i], False] )\n",
    "        # Pairwise comparaison between Histidine of the same helix, marking pairs separated by 12 to 14 residues\n",
    "        for i in range (0, len(H_status) - 1):\n",
    "            for j in range (i + 1, len(H_status)):\n",
    "                if H_status[i][1] != H_status[j][1]:\n",
    "                    continue\n",
    "                d = H_status[i][0] - H_status[j][0]\n",
    "                if d >= 12 or d <= 14:\n",
    "                    H_status[i][2] = True\n",
    "                    H_status[j][2] = True\n",
    "        \n",
    "        #print(H_status)\n",
    "        # Only keep marked histidine\n",
    "        H_status = [ x for x in H_status if x[2] ]\n",
    "        # Create a dicitinary where keys are Helices numbers\n",
    "        H_groups = {}\n",
    "        for x in H_status:\n",
    "            if not x[2]:\n",
    "                continue\n",
    "            if x[1] not in H_groups:\n",
    "                H_groups[x[1]]=[]\n",
    "            H_groups[x[1]].append(x)\n",
    "        \n",
    "        # The test is passed if at least two distinct helices feature at least one correctly spaced histidine pair\n",
    "        # ie : if the helice dictionary has more than 1 entrie\n",
    "        #print(H_status)\n",
    "        #print(\"-->\", H_groups)\n",
    "        HisTestBool = True if len(H_groups) > 1 else False\n",
    "        \n",
    "        data[_id]['Htest']['status'] = HisTestBool\n",
    "        data[_id]['Htest']['data'] = H_groups\n",
    "    return data\n",
    "\n",
    "#m = HIS_clust(d)\n",
    "#print(len([ m[x] for x in m if m[x]['Htest']['status'] ]), len(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, time\n",
    "import time\n",
    "\n",
    "def save(data, tag=None):\n",
    "    saveDir=\"/Volumes/arwen/mobi/group/NOX_CH/pickle_saved\"\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    fTag = \"NOX_annotation_\" + tag + \"_\" if tag else \"NOX_annotation_\"\n",
    "    fSerialDump = fTag + timestr + \".pickle\"\n",
    "    with open(saveDir + '/' + fSerialDump, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print('data structure saved to', saveDir + '/' + fSerialDump)\n",
    "\n",
    "def load(fileName):\n",
    "    saveDir=\"/Volumes/arwen/mobi/group/NOX_CH/pickle_saved\"\n",
    "    d = pickle.load( open(saveDir + \"/\" + fileName, \"rb\" ) )\n",
    "    print(\"restore a annotated container of \", len(d), \"elements\")\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing all data files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing HMMR data\n",
    "NB: There are stdout of 3 consecutive hmmr calls\n",
    "\n",
    "All in a single **data** container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning:: >> tr|A0A0F0DDW2|A0A0F0DDW2_9PSED  Cytochrome B561 OS=Pseudomonas sp. 21 OX=1619948 GN=UB43_27990 PE=4 SV=1\n",
      "   [No individual domains that satisfy reporting thresholds (although complete target did)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyproteinsExt.hmmrContainerFactory as hm\n",
    "import glob\n",
    "dataDir=glob.glob('/Volumes/arwen/mobi/group/NOX_CH/seedSet/work/uniprot_trembl_v*')\n",
    "\n",
    "data = hm.parse(inputFile=dataDir[0] + '/hmmsearch.out')\n",
    "i=0\n",
    "\n",
    "for iDir in dataDir[1:]:\n",
    "    #print(iDir)\n",
    "    data += hm.parse(inputFile=iDir + '/hmmsearch.out')\n",
    "    i += 1\n",
    "    #if i == 1:\n",
    "     #   break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading TMHMM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataTMHMM = {}\n",
    "for lDir in dataDir:\n",
    "    d = loadTMHMM(lDir)\n",
    "    if set( dataTMHMM.keys() ) & set( d.keys() ):\n",
    "        print('doublons')\n",
    "    dataTMHMM.update(d)\n",
    "\n",
    "dataTMHMM = HIS_clust(dataTMHMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform a PFAM domain indexed data structure in a protein indexed data structure\n",
    "Then filter out the protein that feature the 3 domains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins entries featuring FAD 77203\n",
      "Number of proteins entries featuring NAD 121386\n",
      "Number of proteins entries featuring Ferric reductase 59209\n",
      "Size of their intersection 18020\n"
     ]
    }
   ],
   "source": [
    "T = data.T()\n",
    "D = {}\n",
    "fad=0\n",
    "nad=0\n",
    "ferric=0\n",
    "for protein in T:\n",
    "    if len(T[protein]) == 3:\n",
    "           D[protein] = T[protein]\n",
    "    for dom in T[protein]: \n",
    "        if dom == \"PF08022_full\":\n",
    "            fad+=1\n",
    "        elif dom == \"PF01794_full\": \n",
    "            ferric+=1\n",
    "        elif dom == \"PF08030_full\": \n",
    "            nad+=1\n",
    "        else: \n",
    "            print(\"OOOO\")\n",
    "        #if dom == \"PF08022_full\":\n",
    "            \n",
    "print('Number of proteins entries featuring FAD',fad)\n",
    "print('Number of proteins entries featuring NAD',nad)\n",
    "print('Number of proteins entries featuring Ferric reductase',ferric)\n",
    "print('Size of their intersection',len(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge TMHMM & HMMR data\n",
    "\n",
    "  * Proteins with the 3 domain types\n",
    "  * Their TMHMM status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of protein entries featuring FAD,NAD and Ferric transferase domains 18020\n",
      "Number of protein featuring 2 to 7 TMH and 2 bi-histine 178540\n",
      "Size of their intersection 5972\n"
     ]
    }
   ],
   "source": [
    "merged = {}\n",
    "for _id in D:\n",
    "    if _id not in dataTMHMM:\n",
    "        print('Missing protein ID' + _id)\n",
    "    if not dataTMHMM[_id]['Htest']['status']:\n",
    "        continue\n",
    "    merged[_id] = {\n",
    "        'hmmr' : D[_id],\n",
    "        'tmhmm' : dataTMHMM[_id]\n",
    "    }\n",
    "    \n",
    "print('Number of protein entries featuring FAD,NAD and Ferric transferase domains', len(D))\n",
    "print('Number of protein featuring 2 to 7 TMH and 2 bi-histine', len(dataTMHMM))\n",
    "print('Size of their intersection', len(merged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect NCBI Taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract TaxonID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTaxID(datum):\n",
    "    reTaxID = re.compile('OX=([\\d]+)')\n",
    "    m = reTaxID.search(datum['tmhmm']['fasta']['header'])\n",
    "    if not m:\n",
    "        raise ValueError('Cant parse taxid from', datum['tmhmm']['fasta']['header'])\n",
    "    datum['taxid'] = m.groups()[0]\n",
    "    \n",
    "for _id in merged:\n",
    "    getTaxID(merged[_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Flag Non Eukaryota phylum members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import NCBITaxa\n",
    "ncbi=NCBITaxa() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eukaryota 5116\n",
      "Bacteria 848\n",
      "Archaea 3\n",
      "Unclassified 2\n",
      "Not found 3\n"
     ]
    }
   ],
   "source": [
    "unclassified=0\n",
    "archaea=0\n",
    "bacteria=0\n",
    "eukaryota=0\n",
    "not_found=0\n",
    "for _id in merged: \n",
    "    bool=True\n",
    "    taxid=merged[_id]['taxid']\n",
    "    #print(taxid)\n",
    "    try : \n",
    "        lineage=ncbi.get_lineage(taxid)\n",
    "        lineage_rank=ncbi.get_rank(lineage)\n",
    "        superkingdom=[taxid for taxid in lineage_rank if lineage_rank[taxid]=='superkingdom']\n",
    "        if superkingdom : \n",
    "            name=ncbi.get_taxid_translator(superkingdom)[superkingdom[0]]\n",
    "            if name == \"Eukaryota\":\n",
    "                bool=False\n",
    "                eukaryota+=1\n",
    "            elif name == \"Bacteria\":\n",
    "                bacteria+=1\n",
    "            elif name == \"Archaea\": \n",
    "                archaea+=1\n",
    "            else: \n",
    "                print(\"OOO\")\n",
    "        else: \n",
    "            unclassified+=1\n",
    "        merged[_id]['isNoEukaryota']=bool\n",
    "            \n",
    "    except : \n",
    "        not_found+=1\n",
    "\n",
    "print(\"Eukaryota\",eukaryota)\n",
    "print(\"Bacteria\",bacteria)\n",
    "print(\"Archaea\",archaea)\n",
    "print(\"Unclassified\",unclassified)\n",
    "print(\"Not found\", not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cull for prokaryotic proteins (original 996)\n",
    "\n",
    "#### Use proteins as seeds for blast ()\n",
    "\n",
    "#### --> Tree reconstruction\n",
    "\n",
    "#### Additional PFAM annotation\n",
    "\n",
    "#### Sequence clustering\n",
    "\n",
    "#### Profile génétique\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just keep non Eukaryota sequences in datacontainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore a annotated container of  5972 elements\n"
     ]
    }
   ],
   "source": [
    "data=load(\"NOX_annotation_20190506-144039.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data={}\n",
    "for k in data:\n",
    "    if not 'isNoEukaryota' in data[k]:\n",
    "        continue\n",
    "    if data[k]['isNoEukaryota']:\n",
    "        new_data[k] = data[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Save non Eukaryota sequences in given directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "saveDir=\"/Volumes/arwen/mobi/group/NOX_CH/seedSet/NOX_noEukaryota\"\n",
    "def mFastaSplitDump(data, saveDir, fileTag='default' ,distinct=True):\n",
    "    c = 1\n",
    "    f = None\n",
    "    if not distinct:\n",
    "        f = open(saveDir + '/'+ fileTag + '_all.fasta', 'w')\n",
    "        \n",
    "    for _id in data:\n",
    "        if distinct:\n",
    "            f = open(saveDir + '/'+ fileTag + '_' + str(c) + '.fasta', 'w')\n",
    "        c += 1\n",
    "        f.write(data[_id]['tmhmm']['fasta']['header'])\n",
    "        f.write(re.sub(\"(.{81})\", \"\\\\1\\n\", data[_id]['tmhmm']['fasta']['sequence'], 0, re.DOTALL))\n",
    "        if distinct:\n",
    "            f.close()\n",
    "    if not distinct:    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mFastaSplitDump(new_data, saveDir, 'NOX_noEukaryota')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_noEukaryota_20190509-100821.pickle\n"
     ]
    }
   ],
   "source": [
    "save(new_data,\"noEukaryota\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Pfam annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore a annotated container of  853 elements\n"
     ]
    }
   ],
   "source": [
    "data=load(\"NOX_annotation_noEukaryota_20190509-100821.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853 proteins to reannotate\n"
     ]
    }
   ],
   "source": [
    "import pyproteinsExt.hmmrContainerFactory as hm\n",
    "fileName=\"/Volumes/arwen/mobi/group/NOX_CH/seedSet/NOX_noEukaryota_hmmscan.out\"\n",
    "#fileName=\"/tmp/hmmscan.out\"\n",
    "hscan = hm.parse(inputFile=fileName)\n",
    "print( len(hscan.T()), 'proteins to reannotate' )\n",
    "for e in hscan.T():\n",
    "    data[e]['hmmr'] = hscan.T()[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_fullPfam_20190509-101105.pickle\n"
     ]
    }
   ],
   "source": [
    "save(data,\"fullPfam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discard obsolete Uniprot entries, add Uniprot informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore a annotated container of  853 elements\n"
     ]
    }
   ],
   "source": [
    "data=load(\"NOX_annotation_fullPfam_20190509-101105.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing cache location to /Users/chilpert/cache/uniprot\n",
      "Reindexing /Users/chilpert/cache/uniprot\n",
      "Acknowledged 785 entries (/Users/chilpert/cache/uniprot)\n",
      "Changing cache location to /Users/chilpert/cache/pfam\n",
      "Reindexing /Users/chilpert/cache/pfam\n",
      "Acknowledged 612 entries (/Users/chilpert/cache/pfam)\n",
      "got to fetch A0A222ZVW0\n",
      "got to fetch A0A081EJA7\n",
      "got to fetch A0A3G2PAG1\n",
      "got to fetch A0A386VYU3\n",
      "got to fetch A0A386WAT7\n",
      "got to fetch A0A3A3P1S2\n",
      "got to fetch A0A2S8AHE6\n",
      "got to fetch A0A3F2SH26\n",
      "got to fetch A0A3L9B3M0\n",
      "got to fetch A0A3N6YKK6\n",
      "got to fetch A0A3P0ZHT7\n",
      "got to fetch A0A3P1AGH7\n",
      "got to fetch A0A3P0XVR9\n",
      "got to fetch A0A3M8LZQ8\n",
      "got to fetch A0A3M8N5Y4\n",
      "got to fetch A0A3M8MCR7\n",
      "got to fetch A0A3A3L1J7\n",
      "got to fetch A0A3G5XQX4\n",
      "got to fetch A0A1D2XHK9\n",
      "got to fetch A0A2Z5AMG3\n",
      "got to fetch A0A168SHW4\n",
      "got to fetch A0A0C1YSN2\n",
      "got to fetch A0A210TIY3\n",
      "got to fetch A0A1D2YI26\n",
      "got to fetch A0A3N9RMG4\n",
      "got to fetch A0A3G5XBK4\n",
      "got to fetch A0A3G2PKH0\n",
      "got to fetch A0A3N6T799\n",
      "got to fetch A0A085TQG8\n",
      "got to fetch A0A3I2EMP8\n",
      "got to fetch A0A3G9C133\n",
      "got to fetch A0A3I7C7A5\n",
      "got to fetch A0A3K4VJ43\n",
      "got to fetch A0A3J6Z3X5\n",
      "got to fetch A0A3I0QN03\n",
      "got to fetch A0A3K4KS66\n",
      "got to fetch A0A3N1B417\n",
      "got to fetch A0A3N2J7I9\n",
      "got to fetch A0A3N4ZVA0\n",
      "got to fetch A0A3N4WQ35\n",
      "got to fetch A0A3N1WYY3\n",
      "got to fetch A0A3G3NR77\n",
      "got to fetch A0A3G2QJX9\n",
      "got to fetch A0A2S0SF47\n",
      "got to fetch A0A3N9R8D3\n",
      "got to fetch A0A3P1A2C1\n",
      "got to fetch A0A3P0XPF3\n",
      "got to fetch A0A3H5P1D4\n",
      "got to fetch A0A3H7PZM7\n",
      "got to fetch A0A3K6GR70\n",
      "got to fetch A0A3I8WX96\n",
      "got to fetch A0A3I4WQJ9\n",
      "got to fetch A0A3I7WYP1\n",
      "got to fetch A0A3H5ZCN1\n",
      "got to fetch A0A3H7NGN8\n",
      "got to fetch A0A3K1QYH4\n",
      "got to fetch A0A3K5A4C9\n",
      "got to fetch A0A3J8LWU2\n",
      "got to fetch A0A3I2MQS6\n",
      "got to fetch A0A3N1BQ53\n",
      "got to fetch A0A3N1FRV2\n",
      "got to fetch A0A3N4YA67\n",
      "got to fetch A0A3K1VK99\n",
      "got to fetch A0A3L4QTP8\n",
      "got to fetch A0A3L2F2W5\n",
      "got to fetch A0A3L3SJU7\n",
      "got to fetch A0A3L3SPV6\n",
      "got to fetch A0A3I1DF71\n"
     ]
    }
   ],
   "source": [
    "import pyproteinsExt.uniprot as uniprot\n",
    "uColl = uniprot.getUniprotCollection()\n",
    "uColl.setCache(location=\"/Users/chilpert/cache/uniprot\")\n",
    "uniprot.getPfamCollection().setCache(location=\"/Users/chilpert/cache/pfam\")\n",
    "new_data={}\n",
    "c=0\n",
    "not_found=[]\n",
    "for p in data :\n",
    "    p_id=p.split(\"|\")[1]\n",
    "    try : \n",
    "        obj=uColl.get(p_id)\n",
    "        new_data[p]=data[p]\n",
    "        #new_data[p]['RefSeq']={}\n",
    "        #new_data[p]['RefSeq']['genome']=obj.Genome.RefSeqRef\n",
    "        #new_data[p]['RefSeq']['protein']=obj.Genome.RefSeqProteinRef\n",
    "        #new_data[p]['EMBL']={}\n",
    "        #new_data[p]['EMBL']['genome']=obj.Genome.EMBLRef \n",
    "        #new_data[p]['EMBL']['protein']=obj.Genome.EMBLProteinRef\n",
    "        #new_data[p]['Uniprot_domains']=obj.domains\n",
    "        \n",
    "    except : \n",
    "        c+=1\n",
    "        not_found.append(p_id)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785 with Uniprot entry.\n",
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_fullPfam_noObsolete_20190509-112427.pickle\n"
     ]
    }
   ],
   "source": [
    "print(len(new_data),\"with Uniprot entry.\")  \n",
    "save(new_data,\"fullPfam_noObsolete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete domains with evalue > 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore a annotated container of  785 elements\n"
     ]
    }
   ],
   "source": [
    "data=load(\"NOX_annotation_fullPfam_noObsolete_20190509-112427.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAD_binding_6 3.6e-35\n",
      "FAD_binding_8 3e-23\n",
      "FAD_binding_8 1e+04\n",
      "Ferric_reduct 3.8e+03\n",
      "Ferric_reduct 1.9e-16\n",
      "EF-hand_7 9.1e-06\n",
      "EF-hand_1 1.5e+02\n",
      "EF-hand_1 0.00024\n",
      "EF-hand_1 1.1e+04\n",
      "NAD_binding_1 1.5e+04\n",
      "NAD_binding_1 0.0016\n",
      "EF-hand_6 25\n",
      "EF-hand_6 0.084\n",
      "EF-hand_8 0.0013\n",
      "EF-hand_5 0.004\n",
      "FAD_binding_6 0.019\n"
     ]
    }
   ],
   "source": [
    "for d in data['tr|A0A355CGZ4|A0A355CGZ4_9CYAN']['hmmr']: \n",
    "    for h in data['tr|A0A355CGZ4|A0A355CGZ4_9CYAN']['hmmr'][d][0].data : \n",
    "        print(d,h.iEvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_evalue(data,threshold): \n",
    "    new_data={}\n",
    "    for p in data : \n",
    "        keep=False\n",
    "        new_data[p]=data[p].copy()\n",
    "        new_data[p]['hmmr']={}\n",
    "        for d in data[p]['hmmr']:\n",
    "            deleted_hits=0\n",
    "            hits=data[p]['hmmr'][d][0].data\n",
    "            for hit in hits : \n",
    "                evalue=hit.iEvalue\n",
    "                if float(evalue) > threshold : \n",
    "                    deleted_hits+=1\n",
    "            if deleted_hits!=len(hits):\n",
    "                keep=True\n",
    "                new_data[p]['hmmr'][d]=data[p]['hmmr'][d]\n",
    "            if not keep : \n",
    "                print(\"OOOO\")\n",
    "                del new_data[p]  \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data=filter_evalue(data,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_fullPfam_filteredDomains_20190509-113457.pickle\n"
     ]
    }
   ],
   "source": [
    "save(filtered_data,\"fullPfam_filteredDomains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of filtered sequence 386\n",
      "Number of negative to:\n",
      "*The NAD pattern 54 \n",
      "*The FAD pattern 147 \n",
      "*Both patterns  16\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "reMotifNADPH = re.compile('G[ISVL]G[VIAF][TAS][PYTA]')\n",
    "reMotifFAD = re.compile('H[PSA]F[TS][LIMV]')\n",
    "\n",
    "NAD_miss = 0\n",
    "FAD_miss = 0\n",
    "Both_miss = 0\n",
    "for p in merged_restore:\n",
    "    seq = merged_restore[p]['tmhmm']['fasta']['sequence']\n",
    "    m = reMotifNADPH.search(seq)\n",
    "    n = reMotifFAD.search(seq)\n",
    "    merged_restore[p]['NADPH_reg'] = True if m else False\n",
    "    merged_restore[p]['FAD_reg']   = True if n else False\n",
    "\n",
    "    if not m:\n",
    "        NAD_miss += 1\n",
    "        if not n:\n",
    "            Both_miss += 1\n",
    "    if not n:\n",
    "        FAD_miss += 1\n",
    "\n",
    "print('Total Number of filtered sequence', len(merged_restore))\n",
    "print('Number of negative to:')\n",
    "print('*The NAD pattern',str(NAD_miss), '\\n*The FAD pattern', str(FAD_miss), '\\n*Both patterns ', Both_miss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete domains with evalue > 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "data3,c=filter_evalue(data,1e-3)\n",
    "data1,c=filter_evalue(data,1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_domains=set()\n",
    "for p in data3 : \n",
    "    for d in data3[p]['hmmr']: \n",
    "        all_domains.add(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    }
   ],
   "source": [
    "print(len(all_domains))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EF-hand_1', 'EF-hand_7', 'EF-hand_5', 'Fer2', 'EF-hand_8', 'DUF4405', 'FAD_binding_8', 'Ferric_reduct', 'NAD_binding_6', 'NAD_binding_1', 'FAD_binding_6', 'SdpI', 'EF-hand_6', 'DUF2339'}\n"
     ]
    }
   ],
   "source": [
    "print(all_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_fullPfam_filteredDomains1e-3_20190502-184642.pickle\n",
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_fullPfam_filteredDomains1e-1_20190502-184643.pickle\n"
     ]
    }
   ],
   "source": [
    "save(data3,\"fullPfam_filteredDomains1e-3\")\n",
    "save(data1,\"fullPfam_filteredDomains1e-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
