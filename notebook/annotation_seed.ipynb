{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the seed data set\n",
    "\n",
    "Starting from complete trEMBL dataset <span style='background:#f7f3f7;padding:0.4em;border-radius:2px; border:solid bgrey 1px'>arwen:/mobi/group/NOX_CH/data/uniprot_trembl.fasta.gz</span> which is a symbolic link for `arwen:/mobi/group/databases/flat/uniprot_trembl_2019_02.fasta.gz`\n",
    " *  Split the dataset in small volumes\n",
    "     * script: <span style=\"color:green\">**split.py**</span>\n",
    "     * Usage:\n",
    "     Create and go to the `/mobi/group/NOX_GL/volumes` \n",
    "```console\n",
    "    ROOT_DIR=/mobi/group/NOX_CH\n",
    "    SCRIPT_DIR=/mobi/group/NOX_CH/nox-analysis/scripts\n",
    "    $SCRIPT_DIR/split.py $ROOT_DIR/data/uniprot_trembl.fasta.gz\n",
    "```\n",
    "\n",
    " * Run the HMMR and TMHMM annotations\n",
    "    * script: <span style=\"color:green\">**runHMMR_slurm.sh**</span>\n",
    "    * Usage:  \n",
    "  \n",
    "```console\n",
    "    mkdir $ROOT_DIR/seedSet\n",
    "    mkdir $ROOT_DIR/seedSet/work\n",
    "    $SCRIPT_DIR/runHMMR_slurm.sh $ROOT_DIR/volumes $ROOT_DIR/seedSet/work $ROOT_DIR/data/profiles\n",
    "```\n",
    "\n",
    " * Use this notebook to parse the _work_ folder (see **Parsing all data files** section)\n",
    "\n",
    "    * Filter-out non eukaryotic entries and dump the corresponding fasta sequence in folder <span style='background:#f7f3f7;padding:0.4em;border-radius:2px; border:solid bgrey 1px'>/mobi/group/NOX_CH/seedSet/NOX_noEukaryota</span> (create directory before)\n",
    "         \n",
    "\n",
    " * Concatenate all fasta sequences in a single file\n",
    "```console\n",
    "     cd $ROOT_DIR/seedSet/NOX_noEukaryota\n",
    "     for i in $(ls); do sed -i -e '$a\\' $i; done\n",
    "     cat $ROOT_DIR/seedSet/NOX_noEukaryota/*.fasta > $ROOT_DIR/seedSet/NOX_noEukaryota.mfasta\n",
    " ```\n",
    "\n",
    "* Perform full Pfam annotation\n",
    "\n",
    "```console\n",
    "     sbatch $SCRIPT_DIR/runHMMSCAN.sbatch /mobi/group/databases/hmmr/Pfam-A.hmm $ROOT_DIR/seedSet/NOX_noEukaryota.mfasta $ROOT_DIR/seedSet/NOX_noEukaryota_hmmscan.out\n",
    "```\n",
    "\n",
    " * Enrich the datacontainer with these new annotation, see **Full Pfam annotation** section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "import copy\n",
    "sys.path.append(\"/Users/chilpert/Work/pyproteinsExt/src\")\n",
    "sys.path.append(\"/Users/chilpert/Work/pyproteins/src\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, io\n",
    "import urllib.request\n",
    "\n",
    "def mFastaParseZip(inputFile):\n",
    "    data = None\n",
    "    with io.TextIOWrapper(gzip.open(inputFile, 'r')) as f:\n",
    "        data = mFastaParseStream(f)\n",
    "    return data\n",
    "\n",
    "def mFastaParseUrl(url):\n",
    "    fp = urllib.request.urlopen(url)\n",
    "    mybytes = fp.read()\n",
    "    #mFastaParseStream(fp)\n",
    "    mystr = mybytes.decode(\"utf8\")\n",
    "    fp.close()\n",
    "    data = mFastaParseStream(mystr.split('\\n'))\n",
    "    \n",
    "#    print(mystr)\n",
    "    return data\n",
    "\n",
    "def mFastaParseStream(stream):\n",
    "    \n",
    "    data = {}    \n",
    "    headPtr = ''\n",
    "    for line in stream:\n",
    "        #print (line)\n",
    "        if line == '':\n",
    "            continue\n",
    "        s = line.replace('\\n','')\n",
    "        if s.startswith('>'):\n",
    "            headPtr = s.split()[0][1:]\n",
    "            \n",
    "            if headPtr in data:\n",
    "                raise ValueError('Smtg wrong')\n",
    "            data[headPtr] = {'header': s, 'sequence' : '' }\n",
    "            \n",
    "            continue\n",
    "        data[headPtr]['sequence'] += s\n",
    "    return data\n",
    "\n",
    "#mFastaParseUrl('http://www.uniprot.org/uniprot/S4Z6V5.fasta')\n",
    "#data = mFastaParse('/Volumes/arwen/home/ygestin/prositetask-backup/alignTrembl/bibl/Trembl_47/Trembl_47.fasta.gz')\n",
    "#test=None\n",
    "#with open('/Volumes/arwen/mobi/group/NOX_GL/work/uniprot_trembl_v11/hmmsearch.fasta', 'r') as f:\n",
    "#    test = mFastaParseStream(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def num(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return float(s)\n",
    "    \n",
    "    \n",
    "reTMH = re.compile('^(\\# ){0,1}([\\S]+)[\\s]+([\\S].*)[\\s]+([\\d\\.]+)$')\n",
    "def loadTMHMM(lDir):\n",
    "    \n",
    "    fastaContainer = None\n",
    "    with open( lDir+ '/hmmsearch.fasta', 'r') as f:\n",
    "        fastaContainer = mFastaParseStream(f)\n",
    "    \n",
    "    file = lDir+ '/tmhmm.out'\n",
    "    data = {}\n",
    "    with open(file, 'r') as f:\n",
    "        for l in f:\n",
    "            m = reTMH.search(l)\n",
    "            if m:\n",
    "                _id = m.groups()[1] \n",
    "                if _id not in data:\n",
    "                    if _id not in fastaContainer:\n",
    "                        raise ValueError(\"Misisng fasta for tmhmm prediction\")\n",
    "                    data[_id] = {'hCount':0 ,\n",
    "                                'helix':[], 'fasta' : fastaContainer[_id],\n",
    "                                'mask': '-' * len(fastaContainer[_id]['sequence'])\n",
    "                                }\n",
    "                \n",
    "                if not m.groups()[2].startswith('TMHMM2'):\n",
    "                    data[_id][re.sub('[\\s]*:[\\s]*$', '',m.groups()[2])] = num(m.groups()[3])\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                m2 = m.groups()[2].split('\\t')\n",
    "                if not m2:\n",
    "                    raise ValueError('could not parse helix line')\n",
    "                helixCoor =  {'volume' : m2[1], \n",
    "                              'start'  : num(m2[2].replace(' ', '')),\n",
    "                              'stop'   : num(m.groups()[3]) \n",
    "                            }\n",
    "                data[_id]['helix'].append(helixCoor)\n",
    "                \n",
    "                \n",
    "                data[_id]['helix'].append(helixCoor)\n",
    "                #print (data[_id]['mask']) \n",
    "                l_1 = len(data[_id]['mask'])\n",
    "                buf = list(data[_id]['mask'])\n",
    "                symbol = None\n",
    "                if helixCoor['volume'] == 'TMhelix':\n",
    "                    data[_id]['hCount'] += 1\n",
    "                    #symbol = 'H'\n",
    "                    symbol = str(data[_id]['hCount']) if data[_id]['hCount'] < 10 else str(data[_id]['hCount'])[-1]\n",
    "                elif helixCoor['volume'] == 'inside':\n",
    "                    symbol = 'i'\n",
    "                elif helixCoor['volume'] == 'outside':\n",
    "                    symbol = 'e'\n",
    "                else :\n",
    "                    raise ValueError(\"unknown symbol \" + helixCoor['volume'])\n",
    "\n",
    "                i=helixCoor['start'] - 1\n",
    "                j=helixCoor['stop']\n",
    "                #print(i,j,len(buf))\n",
    "                toAdd = symbol * (j - i)\n",
    "                buf[i:j] =  list(toAdd)#helixCoor['stop'] - helixCoor['start'] + 1\n",
    "                data[_id]['mask'] = ''.join(buf)\n",
    "                if len(data[_id]['mask']) != l_1:\n",
    "                    print(\"ERROR \", _id, l_1, len(data[_id]['mask']), '>>', i, j, '<<')\n",
    "                    print (len(buf[i:j]), len(list(toAdd)), symbol, '-->', toAdd )\n",
    "                #print(data[_id]['mask'])\n",
    "    \n",
    "    #        Hcluster(data)\n",
    "    return data\n",
    "#d = loadTMHMM('/Volumes/arwen/home/ygestin/prositetask-backup/alignTrembl/bibl/Trembl_47')\n",
    "#d = loadTMHMM('/Volumes/arwen/mobi/group/NOX_GL/work_sample/uniprot_trembl_v11')\n",
    "#d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HIS_clust(data, min=2, max=7):\n",
    "    for _id in data:\n",
    "        data[_id]['Htest'] = {'status' : False, 'data' : [] }\n",
    "\n",
    "        #Discard unwanted numbe of helices\n",
    "        if data[_id]['hCount'] < min or data[_id]['hCount'] > max:\n",
    "            #print('Wrong helices number ', _id, data[_id]['hCount'])\n",
    "            continue\n",
    "        \n",
    "        H_status = []\n",
    "        iMax = len(data[_id]['mask'])\n",
    "        # internal error check\n",
    "        if len(data[_id]['mask']) != len(data[_id]['fasta']['sequence']) :\n",
    "            print( len(data[_id]['mask']), len(data[_id]['fasta']['sequence']) )\n",
    "            print(_id, data[_id])\n",
    "            raise ValueError(\"\")\n",
    "        # Select only residues that are Histidine within TMH\n",
    "        for i in range(0, iMax):\n",
    "            if data[_id]['mask'][i] == \"i\" or  data[_id]['mask'][i] == \"e\":\n",
    "                continue\n",
    "            if not data[_id]['fasta']['sequence'][i] == \"H\":\n",
    "                continue\n",
    "            H_status.append( [i, data[_id]['mask'][i], False] )\n",
    "        # Pairwise comparaison between Histidine of the same helix, marking pairs separated by 12 to 14 residues\n",
    "        for i in range (0, len(H_status) - 1):\n",
    "            for j in range (i + 1, len(H_status)):\n",
    "                if H_status[i][1] != H_status[j][1]:\n",
    "                    continue\n",
    "                d = H_status[i][0] - H_status[j][0]\n",
    "                if d >= 12 or d <= 14:\n",
    "                    H_status[i][2] = True\n",
    "                    H_status[j][2] = True\n",
    "        \n",
    "        #print(H_status)\n",
    "        # Only keep marked histidine\n",
    "        H_status = [ x for x in H_status if x[2] ]\n",
    "        # Create a dicitinary where keys are Helices numbers\n",
    "        H_groups = {}\n",
    "        for x in H_status:\n",
    "            if not x[2]:\n",
    "                continue\n",
    "            if x[1] not in H_groups:\n",
    "                H_groups[x[1]]=[]\n",
    "            H_groups[x[1]].append(x)\n",
    "        \n",
    "        # The test is passed if at least two distinct helices feature at least one correctly spaced histidine pair\n",
    "        # ie : if the helice dictionary has more than 1 entrie\n",
    "        #print(H_status)\n",
    "        #print(\"-->\", H_groups)\n",
    "        HisTestBool = True if len(H_groups) > 1 else False\n",
    "        \n",
    "        data[_id]['Htest']['status'] = HisTestBool\n",
    "        data[_id]['Htest']['data'] = H_groups\n",
    "    return data\n",
    "\n",
    "#m = HIS_clust(d)\n",
    "#print(len([ m[x] for x in m if m[x]['Htest']['status'] ]), len(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, time\n",
    "import time\n",
    "\n",
    "def save(data, tag=None):\n",
    "    saveDir=\"/Volumes/arwen/mobi/group/NOX_CH/pickle_saved\"\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    fTag = \"NOX_annotation_\" + tag + \"_\" if tag else \"NOX_annotation_\"\n",
    "    fSerialDump = fTag + timestr + \".pickle\"\n",
    "    with open(saveDir + '/' + fSerialDump, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print('data structure saved to', saveDir + '/' + fSerialDump)\n",
    "\n",
    "def load(fileName):\n",
    "    saveDir=\"/Volumes/arwen/mobi/group/NOX_CH/pickle_saved\"\n",
    "    d = pickle.load( open(saveDir + \"/\" + fileName, \"rb\" ) )\n",
    "    print(\"restore a annotated container of \", len(d), \"elements\")\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing all data files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing HMMR data\n",
    "NB: There are stdout of 3 consecutive hmmr calls\n",
    "\n",
    "All in a single **data** container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "Warning:: >> tr|A0A0F0DDW2|A0A0F0DDW2_9PSED  Cytochrome B561 OS=Pseudomonas sp. 21 OX=1619948 GN=UB43_27990 PE=4 SV=1\n",
      "   [No individual domains that satisfy reporting thresholds (although complete target did)]\n",
      "\n",
      "\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n"
     ]
    }
   ],
   "source": [
    "import pyproteinsExt.hmmrContainerFactory as hm\n",
    "import glob\n",
    "dataDir=glob.glob('/Volumes/arwen/mobi/group/NOX_CH/seedSet/work/uniprot_trembl_v*')\n",
    "\n",
    "data = hm.parse(inputFile=dataDir[0] + '/hmmsearch.out')\n",
    "i=0\n",
    "\n",
    "for iDir in dataDir[1:]:\n",
    "    #print(iDir)\n",
    "    data += hm.parse(inputFile=iDir + '/hmmsearch.out')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Discard domains with evalue > 1e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.evalue_filter(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.evalue_filter(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257799\n",
      "98292\n"
     ]
    }
   ],
   "source": [
    "T_all=data.T()\n",
    "T=data.T(filter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proteins with at least 1 domain 178539\n",
      "Proteins with at least 1 domain with evalue filter at 1e-3 64021\n"
     ]
    }
   ],
   "source": [
    "print(\"Proteins with at least 1 domain\",len(T_all))\n",
    "print(\"Proteins with at least 1 domain with evalue filter at 1e-3\",len(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading TMHMM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataTMHMM = {}\n",
    "for lDir in dataDir:\n",
    "    d = loadTMHMM(lDir)\n",
    "    if set( dataTMHMM.keys() ) & set( d.keys() ):\n",
    "        print('doublons')\n",
    "    dataTMHMM.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178540\n"
     ]
    }
   ],
   "source": [
    "dataTMHMM = HIS_clust(dataTMHMM)\n",
    "print(len(dataTMHMM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178540\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10989\n"
     ]
    }
   ],
   "source": [
    "dataFilter=[p for p in dataTMHMM if dataTMHMM[p]['Htest']['status']]\n",
    "print(len(dataFilter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prot=set([p for p in dataFilter])\n",
    "print(len(all_prot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproteinsExt.tmhmmContainerFactory as tmhmm \n",
    "data_tmhmm=tmhmm.parse(inputFile=dataDir[0]+'/tmhmm.out')\n",
    "#print(len(data_tmhmm))\n",
    "for iDir in dataDir[1:]:\n",
    "    data_tmhmm+=tmhmm.parse(inputFile=iDir+'/tmhmm.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prot2=set([obj.prot for obj in data_tmhmm.entries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mFastaParseStream(stream):\n",
    "    data = {}    \n",
    "    headPtr = ''\n",
    "    for line in stream:\n",
    "        #print (line)\n",
    "        if line.startswith(\"#No protein detected by HMMR\"): \n",
    "            return data\n",
    "        if line == '':\n",
    "            continue\n",
    "        s = line.replace('\\n','')\n",
    "        if s.startswith('>'):\n",
    "            headPtr = s.split()[0][1:]\n",
    "            \n",
    "            if headPtr in data:\n",
    "                raise ValueError('Smtg wrong')\n",
    "            data[headPtr] = {'header': s, 'sequence' : '' }\n",
    "            \n",
    "            continue\n",
    "        data[headPtr]['sequence'] += s\n",
    "    return data\n",
    "\n",
    "def parseFasta(lDir): \n",
    "    dic={}\n",
    "    with open( lDir+ '/hmmsearch.fasta', 'r') as f:\n",
    "        fastaContainer = mFastaParseStream(f)\n",
    "        for p in fastaContainer: \n",
    "            dic[p]=fastaContainer[p]\n",
    "            \n",
    "    return dic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFasta={}\n",
    "for lDir in dataDir:\n",
    "    d = parseFasta(lDir)\n",
    "    dataFasta.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56999\n",
      "TO\n",
      "10989\n"
     ]
    }
   ],
   "source": [
    "data_tmhmm.filter_nb_helix(2,7)\n",
    "print(len(data_tmhmm))\n",
    "data_tmhmm.filter(tmhmm.filter_bi_histidine,dataFasta)\n",
    "print(len(data_tmhmm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform a PFAM domain indexed data structure in a protein indexed data structure\n",
    "Then filter out the protein that feature the 3 domains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins entries featuring FAD 77203\n",
      "Number of proteins entries featuring NAD 121386\n",
      "Number of proteins entries featuring Ferric reductase 59209\n",
      "Size of their intersection 18020\n"
     ]
    }
   ],
   "source": [
    "D_all = {}\n",
    "fad=0\n",
    "nad=0\n",
    "ferric=0\n",
    "for protein in T_all:\n",
    "    if len(T_all[protein]) == 3:\n",
    "           D_all[protein] = T_all[protein]\n",
    "    for dom in T_all[protein]: \n",
    "        if dom == \"PF08022_full\":\n",
    "            fad+=1\n",
    "        elif dom == \"PF01794_full\": \n",
    "            ferric+=1\n",
    "        elif dom == \"PF08030_full\": \n",
    "            nad+=1\n",
    "        else: \n",
    "            print(\"OOOO\")\n",
    "        #if dom == \"PF08022_full\":\n",
    "            \n",
    "print('Number of proteins entries featuring FAD',fad)\n",
    "print('Number of proteins entries featuring NAD',nad)\n",
    "print('Number of proteins entries featuring Ferric reductase',ferric)\n",
    "print('Size of their intersection',len(D_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of proteins entries featuring FAD 31593\n",
      "Number of proteins entries featuring NAD 37456\n",
      "Number of proteins entries featuring Ferric reductase 29243\n",
      "Size of their intersection 14022\n"
     ]
    }
   ],
   "source": [
    "D = {}\n",
    "fad=0\n",
    "nad=0\n",
    "ferric=0\n",
    "for protein in T:\n",
    "    if len(T[protein]) == 3:\n",
    "           D[protein] = T[protein]\n",
    "    for dom in T[protein]: \n",
    "        if dom == \"PF08022_full\":\n",
    "            fad+=1\n",
    "        elif dom == \"PF01794_full\": \n",
    "            ferric+=1\n",
    "        elif dom == \"PF08030_full\": \n",
    "            nad+=1\n",
    "        else: \n",
    "            print(\"OOOO\")\n",
    "        #if dom == \"PF08022_full\":\n",
    "            \n",
    "print('Number of proteins entries featuring FAD',fad)\n",
    "print('Number of proteins entries featuring NAD',nad)\n",
    "print('Number of proteins entries featuring Ferric reductase',ferric)\n",
    "print('Size of their intersection',len(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge TMHMM & HMMR data\n",
    "\n",
    "  * Proteins with the 3 domain types\n",
    "  * Their TMHMM status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of protein entries featuring FAD,NAD and Ferric transferase domains 14022\n",
      "Number of protein featuring 2 to 7 TMH and 2 bi-histine 178540\n",
      "Size of their intersection 5972\n"
     ]
    }
   ],
   "source": [
    "merged_all = {}\n",
    "for _id in D_all:\n",
    "    if _id not in dataTMHMM:\n",
    "        print('Missing protein ID' + _id)\n",
    "    if not dataTMHMM[_id]['Htest']['status']:\n",
    "        continue\n",
    "    merged_all[_id] = {\n",
    "        'hmmr' : D_all[_id],\n",
    "        'tmhmm' : dataTMHMM[_id]\n",
    "    }\n",
    "    \n",
    "print('Number of protein entries featuring FAD,NAD and Ferric transferase domains', len(D))\n",
    "print('Number of protein featuring 2 to 7 TMH and 2 bi-histine', len(dataTMHMM))\n",
    "print('Size of their intersection', len(merged_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of protein entries featuring FAD,NAD and Ferric transferase domains 14022\n",
      "Number of protein featuring 2 to 7 TMH and 2 bi-histine 178540\n",
      "Size of their intersection 5043\n"
     ]
    }
   ],
   "source": [
    "merged = {}\n",
    "for _id in D:\n",
    "    if _id not in dataTMHMM:\n",
    "        print('Missing protein ID' + _id)\n",
    "    if not dataTMHMM[_id]['Htest']['status']:\n",
    "        continue\n",
    "    merged[_id] = {\n",
    "        'hmmr' : D[_id],\n",
    "        'tmhmm' : dataTMHMM[_id]\n",
    "    }\n",
    "    \n",
    "print('Number of protein entries featuring FAD,NAD and Ferric transferase domains', len(D))\n",
    "print('Number of protein featuring 2 to 7 TMH and 2 bi-histine', len(dataTMHMM))\n",
    "print('Size of their intersection', len(merged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discard Eukaryota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract TaxonID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTaxID(datum):\n",
    "    reTaxID = re.compile('OX=([\\d]+)')\n",
    "    m = reTaxID.search(datum['tmhmm']['fasta']['header'])\n",
    "    if not m:\n",
    "        raise ValueError('Cant parse taxid from', datum['tmhmm']['fasta']['header'])\n",
    "    datum['taxid'] = m.groups()[0]\n",
    "    \n",
    "for _id in merged:\n",
    "    getTaxID(merged[_id])\n",
    "\n",
    "for _id in merged_all:\n",
    "    getTaxID(merged_all[_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flag Non Eukaryota phylum members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import NCBITaxa\n",
    "ncbi=NCBITaxa() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eukaryota 5116\n",
      "Bacteria 848\n",
      "Archaea 3\n",
      "Unclassified 2\n",
      "Not found 3\n"
     ]
    }
   ],
   "source": [
    "unclassified=0\n",
    "archaea=0\n",
    "bacteria=0\n",
    "eukaryota=0\n",
    "not_found=0\n",
    "for _id in merged_all: \n",
    "    bool=True\n",
    "    taxid=merged_all[_id]['taxid']\n",
    "    #print(taxid)\n",
    "    try : \n",
    "        lineage=ncbi.get_lineage(taxid)\n",
    "        lineage_rank=ncbi.get_rank(lineage)\n",
    "        superkingdom=[taxid for taxid in lineage_rank if lineage_rank[taxid]=='superkingdom']\n",
    "        if superkingdom : \n",
    "            name=ncbi.get_taxid_translator(superkingdom)[superkingdom[0]]\n",
    "            if name == \"Eukaryota\":\n",
    "                bool=False\n",
    "                eukaryota+=1\n",
    "            elif name == \"Bacteria\":\n",
    "                bacteria+=1\n",
    "            elif name == \"Archaea\": \n",
    "                archaea+=1\n",
    "            else: \n",
    "                print(\"OOO\")\n",
    "        else: \n",
    "            unclassified+=1\n",
    "        merged_all[_id]['isNoEukaryota']=bool\n",
    "            \n",
    "    except : \n",
    "        not_found+=1\n",
    "\n",
    "print(\"Eukaryota\",eukaryota)\n",
    "print(\"Bacteria\",bacteria)\n",
    "print(\"Archaea\",archaea)\n",
    "print(\"Unclassified\",unclassified)\n",
    "print(\"Not found\", not_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtered data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eukaryota 4870\n",
      "Bacteria 169\n",
      "Archaea 0\n",
      "Unclassified 1\n",
      "Not found 3\n"
     ]
    }
   ],
   "source": [
    "unclassified=0\n",
    "archaea=0\n",
    "bacteria=0\n",
    "eukaryota=0\n",
    "not_found=0\n",
    "for _id in merged: \n",
    "    bool=True\n",
    "    taxid=merged[_id]['taxid']\n",
    "    #print(taxid)\n",
    "    try : \n",
    "        lineage=ncbi.get_lineage(taxid)\n",
    "        lineage_rank=ncbi.get_rank(lineage)\n",
    "        superkingdom=[taxid for taxid in lineage_rank if lineage_rank[taxid]=='superkingdom']\n",
    "        if superkingdom : \n",
    "            name=ncbi.get_taxid_translator(superkingdom)[superkingdom[0]]\n",
    "            if name == \"Eukaryota\":\n",
    "                bool=False\n",
    "                eukaryota+=1\n",
    "            elif name == \"Bacteria\":\n",
    "                bacteria+=1\n",
    "            elif name == \"Archaea\": \n",
    "                archaea+=1\n",
    "            else: \n",
    "                print(\"OOO\")\n",
    "        else: \n",
    "            unclassified+=1\n",
    "        merged[_id]['isNoEukaryota']=bool\n",
    "            \n",
    "    except : \n",
    "        not_found+=1\n",
    "\n",
    "print(\"Eukaryota\",eukaryota)\n",
    "print(\"Bacteria\",bacteria)\n",
    "print(\"Archaea\",archaea)\n",
    "print(\"Unclassified\",unclassified)\n",
    "print(\"Not found\", not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_filter_20190516-163544.pickle\n"
     ]
    }
   ],
   "source": [
    "save(merged,\"filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just keep non Eukaryota sequences in datacontainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=merged_all\n",
    "data_noEuk_all={}\n",
    "for k in data:\n",
    "    if not 'isNoEukaryota' in data[k]:\n",
    "        continue\n",
    "    if data[k]['isNoEukaryota']:\n",
    "        data_noEuk_all[k] = data[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=merged\n",
    "data_noEuk={}\n",
    "for k in data:\n",
    "    if not 'isNoEukaryota' in data[k]:\n",
    "        continue\n",
    "    if data[k]['isNoEukaryota']:\n",
    "        data_noEuk[k] = data[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853 NOX proteins found with no filter.\n",
      "170 NOX proteins found with evalue filter\n"
     ]
    }
   ],
   "source": [
    "print(len(data_noEuk_all),\"NOX proteins found with no filter.\")\n",
    "print(len(data_noEuk),\"NOX proteins found with evalue filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteins_all=set([p for p in data_noEuk_all])\n",
    "proteins_filtered=set([p for p in data_noEuk])\n",
    "deleted_proteins=proteins_all.difference(proteins_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write deleted proteins with their taxonomy in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "o=open(\"/Volumes/arwen/mobi/group/NOX_CH/deleted_proteins_with_domain_filter.tsv\",'w')\n",
    "o.write(\"#Protein\\tTaxid\\tTaxname\\n\")\n",
    "for p in deleted_proteins: \n",
    "    taxid=data_noEuk_all[p]['taxid']\n",
    "    taxname=ncbi.get_taxid_translator([taxid]).get(int(taxid),\"no name\")\n",
    "    o.write(p+\"\\t\"+taxid+\"\\t\"+taxname+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_filter_noEukaryota_20190516-163551.pickle\n"
     ]
    }
   ],
   "source": [
    "save(data_noEuk,\"filter_noEukaryota\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we work only with filtered proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter obsolete Uniprot entry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore a annotated container of  170 elements\n"
     ]
    }
   ],
   "source": [
    "data=load(\"NOX_annotation_filter_noEukaryota_20190516-163551.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing cache location to /Users/chilpert/cache/uniprot\n",
      "Reindexing /Users/chilpert/cache/uniprot\n",
      "Acknowledged 163 entries (/Users/chilpert/cache/uniprot)\n",
      "Changing cache location to /Users/chilpert/cache/pfam\n",
      "Reindexing /Users/chilpert/cache/pfam\n",
      "Acknowledged 159 entries (/Users/chilpert/cache/pfam)\n",
      "got to fetch A0A3L9B3M0\n",
      "got to fetch A0A2Z5AMG3\n",
      "got to fetch A0A168SHW4\n",
      "got to fetch A0A3L4QTP8\n",
      "got to fetch A0A3L2F2W5\n",
      "got to fetch A0A3L3SJU7\n",
      "got to fetch A0A3L3SPV6\n"
     ]
    }
   ],
   "source": [
    "import pyproteinsExt.uniprot as uniprot\n",
    "uColl = uniprot.getUniprotCollection()\n",
    "uColl.setCache(location=\"/Users/chilpert/cache/uniprot\")\n",
    "uniprot.getPfamCollection().setCache(location=\"/Users/chilpert/cache/pfam\")\n",
    "new_data={}\n",
    "c=0\n",
    "not_found=[]\n",
    "for p in data :\n",
    "    p_id=p.split(\"|\")[1]\n",
    "    try : \n",
    "        obj=uColl.get(p_id)\n",
    "        new_data[p]=data[p]\n",
    "        new_data[p]['RefSeq']={}\n",
    "        new_data[p]['RefSeq']['genome']=obj.Genome.RefSeqRef\n",
    "        new_data[p]['RefSeq']['protein']=obj.Genome.RefSeqProteinRef\n",
    "        new_data[p]['EMBL']={}\n",
    "        new_data[p]['EMBL']['genome']=obj.Genome.EMBLRef \n",
    "        new_data[p]['EMBL']['protein']=obj.Genome.EMBLProteinRef\n",
    "        #new_data[p]['Uniprot_domains']=obj.domains\n",
    "        \n",
    "    except : \n",
    "        c+=1\n",
    "        not_found.append(p_id)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 with Uniprot entry.\n"
     ]
    }
   ],
   "source": [
    "print(len(new_data),\"with Uniprot entry.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_noEukaryota_noObsolete_20190516-172955.pickle\n"
     ]
    }
   ],
   "source": [
    "save(new_data,\"noEukaryota_noObsolete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save non Eukaryota sequences in given directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restore a annotated container of  163 elements\n"
     ]
    }
   ],
   "source": [
    "data=load(\"NOX_annotation_noEukaryota_noObsolete_20190516-172955.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "saveDir=\"/Volumes/arwen/mobi/group/NOX_CH/seedSet/NOX_noEukaryota\"\n",
    "def mFastaSplitDump(data, saveDir, fileTag='default' ,distinct=True):\n",
    "    c = 1\n",
    "    f = None\n",
    "    if not distinct:\n",
    "        f = open(saveDir + '/'+ fileTag + '_all.fasta', 'w')\n",
    "        \n",
    "    for _id in data:\n",
    "        if distinct:\n",
    "            f = open(saveDir + '/'+ fileTag + '_' + str(c) + '.fasta', 'w')\n",
    "        c += 1\n",
    "        f.write(data[_id]['tmhmm']['fasta']['header']+\"\\n\")\n",
    "        f.write(re.sub(\"(.{81})\", \"\\\\1\\n\", data[_id]['tmhmm']['fasta']['sequence'], 0, re.DOTALL))\n",
    "        if distinct:\n",
    "            f.close()\n",
    "    if not distinct:    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mFastaSplitDump(data, saveDir, 'NOX_noEukaryota')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Pfam annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse hmm and discard domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproteinsExt.hmmrContainerFactory as hm\n",
    "\n",
    "def parse_and_discard(data,evalue_threshold=None):\n",
    "    new_data=copy.deepcopy(data)\n",
    "    for p in new_data: \n",
    "        new_data[p]['hmmr']={}\n",
    "    fileName=\"/Volumes/arwen/mobi/group/NOX_CH/seedSet/NOX_noEukaryota_hmmscan.out\" \n",
    "    hscan=hm.parse(inputFile=fileName)\n",
    "    bool_filter=False\n",
    "    if evalue_threshold: \n",
    "        hscan.evalue_filter(evalue_threshold)\n",
    "        bool_filter=True \n",
    "    transpose=hscan.T(filter=bool_filter)    \n",
    "    for e in transpose: \n",
    "        new_data[e]['hmmr']=transpose[e]\n",
    "        \n",
    "    print(new_data['tr|A0A2M7JHP3|A0A2M7JHP3_9DELT']['hmmr'].keys())\n",
    "    return new_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['NAD_binding_1', 'Ferric_reduct', 'FAD_binding_8', 'NAD_binding_6', 'FAD_binding_6'])\n",
      "dict_keys(['NAD_binding_1', 'Ferric_reduct', 'FAD_binding_8', 'NAD_binding_6', 'FAD_binding_6'])\n",
      "dict_keys(['NAD_binding_1', 'Ferric_reduct', 'FAD_binding_8', 'NAD_binding_6', 'FAD_binding_6'])\n"
     ]
    }
   ],
   "source": [
    "# Evalue threshold 1e-3\n",
    "data10_3=parse_and_discard(data,1e-3)\n",
    "# Evalue threshold 1e-1\n",
    "data10_1=parse_and_discard(data,1e-1)\n",
    "# All \n",
    "data_all=parse_and_discard(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_filter_fullPfam_filteredDomains1e-3_20190517-162404.pickle\n",
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_filter_fullPfam_filteredDomains1e-1_20190517-162404.pickle\n",
      "data structure saved to /Volumes/arwen/mobi/group/NOX_CH/pickle_saved/NOX_annotation_filter_fullPfam_allDomains_20190517-162405.pickle\n"
     ]
    }
   ],
   "source": [
    "save(data10_3,\"filter_fullPfam_filteredDomains1e-3\")\n",
    "save(data10_1,\"filter_fullPfam_filteredDomains1e-1\")\n",
    "save(data_all,\"filter_fullPfam_allDomains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of filtered sequence 386\n",
      "Number of negative to:\n",
      "*The NAD pattern 54 \n",
      "*The FAD pattern 147 \n",
      "*Both patterns  16\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "reMotifNADPH = re.compile('G[ISVL]G[VIAF][TAS][PYTA]')\n",
    "reMotifFAD = re.compile('H[PSA]F[TS][LIMV]')\n",
    "\n",
    "NAD_miss = 0\n",
    "FAD_miss = 0\n",
    "Both_miss = 0\n",
    "for p in merged_restore:\n",
    "    seq = merged_restore[p]['tmhmm']['fasta']['sequence']\n",
    "    m = reMotifNADPH.search(seq)\n",
    "    n = reMotifFAD.search(seq)\n",
    "    merged_restore[p]['NADPH_reg'] = True if m else False\n",
    "    merged_restore[p]['FAD_reg']   = True if n else False\n",
    "\n",
    "    if not m:\n",
    "        NAD_miss += 1\n",
    "        if not n:\n",
    "            Both_miss += 1\n",
    "    if not n:\n",
    "        FAD_miss += 1\n",
    "\n",
    "print('Total Number of filtered sequence', len(merged_restore))\n",
    "print('Number of negative to:')\n",
    "print('*The NAD pattern',str(NAD_miss), '\\n*The FAD pattern', str(FAD_miss), '\\n*Both patterns ', Both_miss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
