{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the seed data set\n",
    "\n",
    "Starting from complete trEMBL dataset <span style='background:#f7f3f7;padding:0.4em;border-radius:2px; border:solid bgrey 1px'>arwen:/mobi/group/NOX_GL/data/uniprot_trembl.fasta.gz</span>\n",
    " *  Split the dataset in small volumes\n",
    "     * script: <span style=\"color:green\">**split.py**</span>\n",
    "     * Usage:\n",
    "     Go to the `/mobi/group/NOX_GL/volumes` \n",
    "```console\n",
    "    ROOT_DIR=/mobi/group/NOX_GL\n",
    "    $ROOT_DIR/scripts/split.py $ROOT_DIR/data/uniprot_trembl.fasta.gz\n",
    "```\n",
    "\n",
    " * Run the HMMR and TMHMM annotations\n",
    "    * script: <span style=\"color:green\">**runHMMR_slurm.sh**</span>\n",
    "    * Usage:  \n",
    "  \n",
    "```console\n",
    "    $ROOT_DIR/scripts/runHMMR_slurm.sh $ROOT_DIR/seedSet/work $ROOT_DIR/volumes $ROOT_DIR/data/profiles\n",
    "```\n",
    "\n",
    " * Use this notebook to parse the _work_ folder (see **Parsing all data files** section)\n",
    "\n",
    "    * Filter-out non eukaryotic entries and dump the corresponding fasta sequence in folder <span style='background:#f7f3f7;padding:0.4em;border-radius:2px; border:solid bgrey 1px'>/mobi/group/NOX_GL/seedSet/NOX_noEukaryota</span>\n",
    "\n",
    "\n",
    " * Preparing folders/sbatch scripts for pairwise N&W across the set of __NOX_noEukaryota__ fasta sequences\n",
    "    * script: <span style=\"color:green\">**runEMBOSS_slurm.sh**</span>\n",
    "    * Usage:\n",
    "```console\n",
    "$ROOT_DIR/scripts/runEMBOSS_slurm.sh $ROOT_DIR/seedSet/NOX_noEukaryota NOX_noEukaryota $ROOT_DIR/seedSet/NOX_noEukaryota_needlePairwise_work\n",
    "```\n",
    "\n",
    " * Concatenate all fasta sequences in a single file and perform a full PFAM annotation\n",
    "```console\n",
    "     cat $ROOT_DIR/seedSet/NOX_noEukaryota/*.fasta > NOX_noEukaryota.mfasta\n",
    "     hmmscan NOX_noEukaryota.mfasta /mobi/group/databases/hmmr/Pfam-A.hmm > NOX_noEukaryota_hmmscan.out\n",
    "```\n",
    "\n",
    "\n",
    " * Enrich the datacontainer with these new annotation, see **Read in additional PFAM annotations // Erase previous** section\n",
    "\n",
    "\n",
    " * Use the [Taxonomy notebook](http://localhost:8888/notebooks/NOX/Taxonomy.ipynb) to output a hierarchal tree\n",
    "     * link the output json file as $latest.json$\n",
    "\n",
    "\n",
    " * Start adhoc http server\n",
    "   \n",
    "   Go to `~/work/projects/NOX`\n",
    "```console\n",
    "node index.js\n",
    "``` \n",
    "\n",
    "* Visualize w/ D3 at `localhost:9615`\n",
    " \n",
    "### Creating the extended data set\n",
    "\n",
    "\n",
    "* Perform a psiblast on fasta files present in <span style='background:#f7f3f7;padding:0.4em;border-radius:2px; border:solid bgrey 1px'>arwen:/mobi/group/NOX_GL/seedSet/NOX_noEukaryota</span>\n",
    "\n",
    "    * Create the `extendedSet` folder\n",
    "\n",
    "    * script: <span style=\"color:green\">**runPsiBlast_slurm.sh**</span>\n",
    "    * Usage:\n",
    "```console\n",
    "$ROOT/scripts/runPsiBlast_slurm.sh $ROOT/seedSet/NOX_noEukaryota $ROOT/extendedSet/psiblastWork\n",
    "```\n",
    "\n",
    "* Browse all the psiblast workfolder and eliminate strictly identical proteins\n",
    "    * Go to `$ROOT/extendedSet`\n",
    "    * script:<span style=\"color:green\">**makePsiBlastNR.py**</span>\n",
    "    * Usage:\n",
    "```console\n",
    "$ROOT/scripts/makePsiBlastNR.py ./psiblastWork ./NOX_noEukaryota_PB_NR.fasta > makePsiBlastNR.log\n",
    "```\n",
    "* Perform a full PFAM annotation\n",
    "```console\n",
    "hmmscan NOX_noEukaryota_PB_NR.fasta /mobi/group/databases/hmmr/Pfam-A.hmm > NOX_noEukaryota_PB_NR_hmmscan.out\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "sys.path.append(\"/Users/guillaumelaunay/work/DVL/python3/pyproteinsExt/src\")\n",
    "sys.path.append(\"/Users/guillaumelaunay/work/DVL/python3/pyproteins/src\")\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip, io\n",
    "import urllib.request\n",
    "\n",
    "def mFastaParseZip(inputFile):\n",
    "    data = None\n",
    "    with io.TextIOWrapper(gzip.open(inputFile, 'r')) as f:\n",
    "        data = mFastaParseStream(f)\n",
    "    return data\n",
    "\n",
    "def mFastaParseUrl(url):\n",
    "    fp = urllib.request.urlopen(url)\n",
    "    mybytes = fp.read()\n",
    "    #mFastaParseStream(fp)\n",
    "    mystr = mybytes.decode(\"utf8\")\n",
    "    fp.close()\n",
    "    data = mFastaParseStream(mystr.split('\\n'))\n",
    "    \n",
    "#    print(mystr)\n",
    "    return data\n",
    "\n",
    "def mFastaParseStream(stream):\n",
    "    \n",
    "    data = {}    \n",
    "    headPtr = ''\n",
    "    for line in stream:\n",
    "        #print (line)\n",
    "        if line == '':\n",
    "            continue\n",
    "        s = line.replace('\\n','')\n",
    "        if s.startswith('>'):\n",
    "            headPtr = s.split()[0][1:]\n",
    "            \n",
    "            if headPtr in data:\n",
    "                raise ValueError('Smtg wrong')\n",
    "            data[headPtr] = {'header': s, 'sequence' : '' }\n",
    "            \n",
    "            continue\n",
    "        data[headPtr]['sequence'] += s\n",
    "    return data\n",
    "\n",
    "#mFastaParseUrl('http://www.uniprot.org/uniprot/S4Z6V5.fasta')\n",
    "#data = mFastaParse('/Volumes/arwen/home/ygestin/prositetask-backup/alignTrembl/bibl/Trembl_47/Trembl_47.fasta.gz')\n",
    "#test=None\n",
    "#with open('/Volumes/arwen/mobi/group/NOX_GL/work/uniprot_trembl_v11/hmmsearch.fasta', 'r') as f:\n",
    "#    test = mFastaParseStream(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def num(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return float(s)\n",
    "    \n",
    "    \n",
    "reTMH = re.compile('^(\\# ){0,1}([\\S]+)[\\s]+([\\S].*)[\\s]+([\\d\\.]+)$')\n",
    "def loadTMHMM(lDir):\n",
    "    \n",
    "    fastaContainer = None\n",
    "    with open( lDir+ '/hmmsearch.fasta', 'r') as f:\n",
    "        fastaContainer = mFastaParseStream(f)\n",
    "    \n",
    "    file = lDir+ '/tmhmm.out'\n",
    "    data = {}\n",
    "    with open(file, 'r') as f:\n",
    "        for l in f:\n",
    "            m = reTMH.search(l)\n",
    "            if m:\n",
    "                _id = m.groups()[1] \n",
    "                if _id not in data:\n",
    "                    if _id not in fastaContainer:\n",
    "                        raise ValueError(\"Misisng fasta for tmhmm prediction\")\n",
    "                    data[_id] = {'hCount':0 ,\n",
    "                                'helix':[], 'fasta' : fastaContainer[_id],\n",
    "                                'mask': '-' * len(fastaContainer[_id]['sequence'])\n",
    "                                }\n",
    "                \n",
    "                if not m.groups()[2].startswith('TMHMM2'):\n",
    "                    data[_id][re.sub('[\\s]*:[\\s]*$', '',m.groups()[2])] = num(m.groups()[3])\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                m2 = m.groups()[2].split('\\t')\n",
    "                if not m2:\n",
    "                    raise ValueError('could not parse helix line')\n",
    "                helixCoor =  {'volume' : m2[1], \n",
    "                              'start'  : num(m2[2].replace(' ', '')),\n",
    "                              'stop'   : num(m.groups()[3]) \n",
    "                            }\n",
    "                data[_id]['helix'].append(helixCoor)\n",
    "                \n",
    "                \n",
    "                data[_id]['helix'].append(helixCoor)\n",
    "                #print (data[_id]['mask']) \n",
    "                l_1 = len(data[_id]['mask'])\n",
    "                buf = list(data[_id]['mask'])\n",
    "                symbol = None\n",
    "                if helixCoor['volume'] == 'TMhelix':\n",
    "                    data[_id]['hCount'] += 1\n",
    "                    #symbol = 'H'\n",
    "                    symbol = str(data[_id]['hCount']) if data[_id]['hCount'] < 10 else str(data[_id]['hCount'])[-1]\n",
    "                elif helixCoor['volume'] == 'inside':\n",
    "                    symbol = 'i'\n",
    "                elif helixCoor['volume'] == 'outside':\n",
    "                    symbol = 'e'\n",
    "                else :\n",
    "                    raise ValueError(\"unknown symbol \" + helixCoor['volume'])\n",
    "\n",
    "                i=helixCoor['start'] - 1\n",
    "                j=helixCoor['stop']\n",
    "                #print(i,j,len(buf))\n",
    "                toAdd = symbol * (j - i)\n",
    "                buf[i:j] =  list(toAdd)#helixCoor['stop'] - helixCoor['start'] + 1\n",
    "                data[_id]['mask'] = ''.join(buf)\n",
    "                if len(data[_id]['mask']) != l_1:\n",
    "                    print(\"ERROR \", _id, l_1, len(data[_id]['mask']), '>>', i, j, '<<')\n",
    "                    print (len(buf[i:j]), len(list(toAdd)), symbol, '-->', toAdd )\n",
    "                #print(data[_id]['mask'])\n",
    "    \n",
    "    #        Hcluster(data)\n",
    "    return data\n",
    "#d = loadTMHMM('/Volumes/arwen/home/ygestin/prositetask-backup/alignTrembl/bibl/Trembl_47')\n",
    "#d = loadTMHMM('/Volumes/arwen/mobi/group/NOX_GL/work_sample/uniprot_trembl_v11')\n",
    "#d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def HIS_clust(data, min=2, max=7):\n",
    "    for _id in data:\n",
    "        data[_id]['Htest'] = {'status' : False, 'data' : [] }\n",
    "\n",
    "        #Discard unwanted numbe of helices\n",
    "        if data[_id]['hCount'] < min or data[_id]['hCount'] > max:\n",
    "            print('Wrong helices number ', _id, data[_id]['hCount'])\n",
    "            continue\n",
    "        \n",
    "        H_status = []\n",
    "        iMax = len(data[_id]['mask'])\n",
    "        # internal error check\n",
    "        if len(data[_id]['mask']) != len(data[_id]['fasta']['sequence']) :\n",
    "            print( len(data[_id]['mask']), len(data[_id]['fasta']['sequence']) )\n",
    "            print(_id, data[_id])\n",
    "            raise ValueError(\"\")\n",
    "        # Select only residues that are Histidine within TMH\n",
    "        for i in range(0, iMax):\n",
    "            if data[_id]['mask'][i] == \"i\" or  data[_id]['mask'][i] == \"e\":\n",
    "                continue\n",
    "            if not data[_id]['fasta']['sequence'][i] == \"H\":\n",
    "                continue\n",
    "            H_status.append( [i, data[_id]['mask'][i], False] )\n",
    "        # Pairwise comparaison between Histidine of the same helix, marking pairs separated by 12 to 14 residues\n",
    "        for i in range (0, len(H_status) - 1):\n",
    "            for j in range (i + 1, len(H_status)):\n",
    "                if H_status[i][1] != H_status[j][1]:\n",
    "                    continue\n",
    "                d = H_status[i][0] - H_status[j][0]\n",
    "                if d >= 12 or d <= 14:\n",
    "                    H_status[i][2] = True\n",
    "                    H_status[j][2] = True\n",
    "        \n",
    "        #print(H_status)\n",
    "        # Only keep marked histidine\n",
    "        H_status = [ x for x in H_status if x[2] ]\n",
    "        # Create a dicitinary where keys are Helices numbers\n",
    "        H_groups = {}\n",
    "        for x in H_status:\n",
    "            if not x[2]:\n",
    "                continue\n",
    "            if x[1] not in H_groups:\n",
    "                H_groups[x[1]]=[]\n",
    "            H_groups[x[1]].append(x)\n",
    "        \n",
    "        # The test is passed if at least two distinct helices feature at least one correctly spaced histidine pair\n",
    "        # ie : if the helice dictionary has more than 1 entrie\n",
    "        #print(H_status)\n",
    "        #print(\"-->\", H_groups)\n",
    "        HisTestBool = True if len(H_groups) > 1 else False\n",
    "        \n",
    "        data[_id]['Htest']['status'] = HisTestBool\n",
    "        data[_id]['Htest']['data'] = H_groups\n",
    "    return data\n",
    "\n",
    "#m = HIS_clust(d)\n",
    "#print(len([ m[x] for x in m if m[x]['Htest']['status'] ]), len(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing all data files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing HMMR data\n",
    "NB: There are stdout of 3 consecutive hmmr calls\n",
    "\n",
    "All in a single **data** container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyproteinsExt.hmmrContainer as hm\n",
    "import glob\n",
    "dataDir=glob.glob('/Volumes/arwen/mobi/group/NOX_GL/seedSet/work/uniprot_trembl_v*')\n",
    "\n",
    "data = hm.parse(inputFile=dataDir[0] + '/hmmsearch.out')\n",
    "i=0\n",
    "for iDir in dataDir[1:]:\n",
    "    print(iDir)\n",
    "    data += hm.parse(inputFile=iDir + '/hmmsearch.out')\n",
    "    i += 1\n",
    "    #if i == 10:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading TMHMM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataTMHMM = {}\n",
    "for lDir in dataDir:\n",
    "    d = loadTMHMM(lDir)\n",
    "    if set( dataTMHMM.keys() ) & set( d.keys() ):\n",
    "        print('doublons')\n",
    "    dataTMHMM.update(d)\n",
    "\n",
    "dataTMHMM = HIS_clust(dataTMHMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Transform a PFAM domain indexed data structure in a protein indexed data structure\n",
    "Then filter out the protein that feature the 3 domains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = data.T()\n",
    "D = {}\n",
    "for protein in T:\n",
    "    if len(T[protein]) == 3:\n",
    "           D[protein] = T[protein]\n",
    "print('Number of protein entries featuring FAD,NAD and Ferric transferase domains', len(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge TMHMM & HMMR data\n",
    "\n",
    "  * Proteins with the 3 domain types\n",
    "  * Their TMHMM status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = {}\n",
    "for _id in D:\n",
    "    if _id not in dataTMHMM:\n",
    "        print('Missing protein ID' + _id)\n",
    "    if not dataTMHMM[_id]['Htest']['status']:\n",
    "        continue\n",
    "    merged[_id] = {\n",
    "        'hmmr' : D[_id],\n",
    "        'tmhmm' : dataTMHMM[_id]\n",
    "    }\n",
    "    \n",
    "print('Number of protein entries featuring FAD,NAD and Ferric transferase domains', len(D))\n",
    "print('Number of protein featuring 2 to 7 TMH and 2 bi-histine', len(dataTMHMM))\n",
    "print('Size of their intersection', len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Split file and do pairwise ali w/ EMBOSS\n",
    "import re\n",
    "saveDir=\"/Volumes/arwen/mobi/group/NOX_GL/seedSet/NOX_noEukaryota\"\n",
    "def mFastaSplitDump(data, saveDir, fileTag='default' ,distinct=True):\n",
    "    c = 1\n",
    "    f = None\n",
    "    if not distinct:\n",
    "        f = open(saveDir + '/'+ fileTag + '_all.fasta', 'w')\n",
    "        \n",
    "    for _id in data:\n",
    "        if distinct:\n",
    "            f = open(saveDir + '/'+ fileTag + '_' + str(c) + '.fasta', 'w')\n",
    "        c += 1\n",
    "        f.write(data[_id]['tmhmm']['fasta']['header'])\n",
    "        f.write(re.sub(\"(.{81})\", \"\\\\1\\n\", data[_id]['tmhmm']['fasta']['sequence'], 0, re.DOTALL))\n",
    "        if distinct:\n",
    "            f.close()\n",
    "    if not distinct:    \n",
    "        f.close()\n",
    "\n",
    "d = {}\n",
    "for k in merged_clone:\n",
    "    if not 'isNoEukaryota' in merged_clone[k]:\n",
    "        continue\n",
    "    if merged_clone[k]['isNoEukaryota']:\n",
    "        d[k] = merged_clone[k]\n",
    "        \n",
    "mFastaSplitDump(d, saveDir, 'NOX_noEukaryota')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect NCBI Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyproteinsExt.ontology\n",
    "taxonTree = pyproteinsExt.ontology.Ontology(file='/Users/guillaumelaunay/work/databases/ontology/ncbitaxon.owl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract TaxonID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def getTaxID(datum):\n",
    "    reTaxID = re.compile('OX=([\\d]+)')\n",
    "    m = reTaxID.search(datum['tmhmm']['fasta']['header'])\n",
    "    if not m:\n",
    "        raise ValueError('Cant parse taxid from', datum['tmhmm']['fasta']['header'])\n",
    "    datum['taxid'] = m.groups()[0]\n",
    "    \n",
    "for _id in merged:\n",
    "    getTaxID(merged[_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Flag Non Eukaryota phylum members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "u = 0\n",
    "for _id in merged:\n",
    "    u += 1\n",
    "    taxid=merged[_id]['taxid']\n",
    "    n = taxonTree.onto.search(iri='http://purl.obolibrary.org/obo/NCBITaxon_' + taxid)\n",
    "    if not n:\n",
    "        print ('Cant find Taxon node for', taxid)\n",
    "        continue\n",
    "\n",
    "    bool=True\n",
    "    for t in taxonTree._getLineage(n[0]):\n",
    "        if not t.label:\n",
    "            continue\n",
    "        if t.label[0] == 'Eukaryota':\n",
    "            bool=False\n",
    "            break\n",
    "    if bool:\n",
    "        cnt += 1\n",
    "    merged[_id]['isNoEukaryota'] = bool      \n",
    "\n",
    "print(\"Total number of bacterial sequences\", cnt, u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Cull for prokaryotic proteins (original 996)\n",
    "\n",
    "#### Use proteins as seeds for blast ()\n",
    "\n",
    "#### --> Tree reconstruction\n",
    "\n",
    "#### Additional PFAM annotation\n",
    "\n",
    "#### Sequence clustering\n",
    "\n",
    "#### Profile génétique\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### De/Serialize the data structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle, time\n",
    "import time\n",
    "\n",
    "def save(data, tag=None):\n",
    "    saveDir=\"/Users/guillaumelaunay/work/projects/NOX\"\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    fTag = \"NOX_annotation_\" + tag + \"_\" if tag else \"NOX_annotation_\"\n",
    "    fSerialDump = fTag + timestr + \".pickle\"\n",
    "    with open(saveDir + '/' + fSerialDump, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print('data structure saved to', saveDir + '/' + fSerialDump)\n",
    "\n",
    "def load(fileName):\n",
    "    saveDir=\"/Users/guillaumelaunay/work/projects/NOX\"\n",
    "    d = pickle.load( open(saveDir + \"/\" + fileName, \"rb\" ) )\n",
    "    print(\"restore a annotated container of \", len(d), \"elements\")\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in additional PFAM annotations // Erase previous\n",
    "  1. restore annotated data structure\n",
    "  1. import a complete PFAM scan of \"isNoEukaryota\" entries\n",
    "  2. replace the 'hmmr' field w/ this one\n",
    "  3. pickle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_restore = load('NOX_annotation_20180619-165729.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_restore['tr|A0A136KU56|A0A136KU56_9CHLR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import pyproteinsExt.hmmrContainerFactory as hm\n",
    "fileName=\"/Volumes/arwen/mobi/group/NOX_GL/toEMBOSS_hmmscan.out\"\n",
    "#fileName=\"/tmp/hmmscan.out\"\n",
    "hscan = hm.parse(inputFile=fileName)\n",
    "print( len(hscan.T()), 'proteins to reannotate' )\n",
    "for e in hscan.T():\n",
    "    merged_restore[e]['hmmr'] = hscan.T()[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save(merged_restore, tag='fullPFAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing to regExp detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "reMotifNADPH = re.compile('G[ISVL]G[VIAF][TAS][PYTA]')\n",
    "reMotifFAD = re.compile('H[PSA]F[TS][LIMV]')\n",
    "\n",
    "NAD_miss = 0\n",
    "FAD_miss = 0\n",
    "Both_miss = 0\n",
    "for p in merged:\n",
    "    seq = merged[p]['tmhmm']['fasta']['sequence']\n",
    "    m = reMotifNADPH.search(seq)\n",
    "    n = reMotifFAD.search(seq)\n",
    "    merged[p]['NADPH_reg'] = True if m else False\n",
    "    merged[p]['FAD_reg']   = True if n else False\n",
    "\n",
    "    if not m:\n",
    "        NAD_miss += 1\n",
    "        if not n:\n",
    "            Both_miss += 1\n",
    "    if not n:\n",
    "        FAD_miss += 1\n",
    "\n",
    "print('Total Number of filtered sequence', len(merged))\n",
    "print('Number of negative to:')\n",
    "print('*The NAD pattern',str(NAD_miss), '\\n*The FAD pattern', str(FAD_miss), '\\n*Both patterns ', Both_miss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3 (p=3.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
